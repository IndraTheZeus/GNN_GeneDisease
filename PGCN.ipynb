{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ca77e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     active environment : GNNGeneDisease\n",
      "    active env location : /home/indradutta/miniconda3/envs/GNNGeneDisease\n",
      "            shell level : 2\n",
      "       user config file : /home/indradutta/.condarc\n",
      " populated config files : \n",
      "          conda version : 23.9.0\n",
      "    conda-build version : not installed\n",
      "         python version : 3.11.4.final.0\n",
      "       virtual packages : __archspec=1=x86_64\n",
      "                          __cuda=12.0=0\n",
      "                          __glibc=2.35=0\n",
      "                          __linux=6.2.0=0\n",
      "                          __unix=0=0\n",
      "       base environment : /home/indradutta/miniconda3  (writable)\n",
      "      conda av data dir : /home/indradutta/miniconda3/etc/conda\n",
      "  conda av metadata url : None\n",
      "           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64\n",
      "                          https://repo.anaconda.com/pkgs/main/noarch\n",
      "                          https://repo.anaconda.com/pkgs/r/linux-64\n",
      "                          https://repo.anaconda.com/pkgs/r/noarch\n",
      "          package cache : /home/indradutta/miniconda3/pkgs\n",
      "                          /home/indradutta/.conda/pkgs\n",
      "       envs directories : /home/indradutta/miniconda3/envs\n",
      "                          /home/indradutta/.conda/envs\n",
      "               platform : linux-64\n",
      "             user-agent : conda/23.9.0 requests/2.31.0 CPython/3.11.4 Linux/6.2.0-36-generic ubuntu/22.04.2 glibc/2.35\n",
      "                UID:GID : 1119:1128\n",
      "             netrc file : None\n",
      "           offline mode : False\n",
      "\n",
      "\n",
      "data_prioritization\t\t model\t PGCN.ipynb\t README.md\n",
      "Disease_gene_prioritization_GCN  output  prediction.npy\n"
     ]
    }
   ],
   "source": [
    "!conda info #Working Environment is GNNGeneDisease\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "890bb3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 12:29:16.002492: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-24 12:29:16.002526: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-24 12:29:16.002565: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from operator import itemgetter\n",
    "from itertools import combinations\n",
    "import time\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as sio\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ce2004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 12:29:18.830585: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 12:29:18.836235: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-24 12:29:18.836437: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "924ddd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5e92f",
   "metadata": {},
   "source": [
    "# DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6043fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD Disease Gene Adjacency Network\n",
    "gene_phenes_path = './data_prioritization/genes_phenes.mat'\n",
    "f = h5py.File(gene_phenes_path, 'r')\n",
    "gene_network_adj = sp.csc_matrix((np.array(f['GeneGene_Hs']['data']),\n",
    "    np.array(f['GeneGene_Hs']['ir']), np.array(f['GeneGene_Hs']['jc'])),\n",
    "    shape=(12331,12331))\n",
    "gene_network_adj = gene_network_adj.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae15f0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "875c7bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_edge_threshold(network_adj, threshold):\n",
    "    edge_tmp, edge_value, shape_tmp = sparse_to_tuple(network_adj)\n",
    "    preserved_edge_index = np.where(edge_value>threshold)[0]\n",
    "    preserved_network = sp.csr_matrix(\n",
    "        (edge_value[preserved_edge_index], \n",
    "        (edge_tmp[preserved_edge_index,0], edge_tmp[preserved_edge_index, 1])),\n",
    "        shape=shape_tmp)\n",
    "    return preserved_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc999264",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_network_adj = sp.csc_matrix((np.array(f['PhenotypeSimilarities']['data']),\n",
    "    np.array(f['PhenotypeSimilarities']['ir']), np.array(f['PhenotypeSimilarities']['jc'])),\n",
    "    shape=(3215, 3215))\n",
    "disease_network_adj = disease_network_adj.tocsr()  ## CONVERTED TO SPARSE ROW\n",
    "# >0.2 values get preserved\n",
    "disease_network_adj = network_edge_threshold(disease_network_adj, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75a032a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: Gene Disease Adj is 1, 0 matrix, while gene_gene (12331, 12331)\n",
    "##        and disease-disease ( 3215, 3215) have edge level values\n",
    "\n",
    "dg_ref = f['GenePhene'][0][0]\n",
    "gene_disease_adj = sp.csc_matrix((np.array(f[dg_ref]['data']),\n",
    "    np.array(f[dg_ref]['ir']), np.array(f[dg_ref]['jc'])),\n",
    "    shape=(12331, 3215))\n",
    "gene_disease_adj = gene_disease_adj.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1fe014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WHAT DATA is THIS ? 34 novel associtaions of value 1, 0\n",
    "novel_associations_adj = sp.csc_matrix((np.array(f['NovelAssociations']['data']),\n",
    "    np.array(f['NovelAssociations']['ir']), np.array(f['NovelAssociations']['jc'])),\n",
    "    shape=(12331,3215))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9faec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature value of each Gene: 4536 features per gene\n",
    "gene_feature_path = './Disease_gene_prioritization_GCN/data_prioritization/GeneFeatures.mat'\n",
    "f_gene_feature = h5py.File(gene_feature_path,'r')\n",
    "gene_feature_exp = np.array(f_gene_feature['GeneFeatures'])\n",
    "gene_feature_exp = np.transpose(gene_feature_exp)\n",
    "gene_network_exp = sp.csc_matrix(gene_feature_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d2cfb7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# // TODO: Explore why 1 - 9 and not 0 - 9\n",
    "## NOTE THIS DATA is different from gene_disease_adj  \n",
    "\n",
    "row_list = [3215, 1137, 744, 2503, 1143, 324, 1188, 4662, 1243]\n",
    "gene_feature_list_other_spe = list()\n",
    "for i in range(1,9):\n",
    "    dg_ref = f['GenePhene'][i][0]\n",
    "    disease_gene_adj_tmp = sp.csc_matrix((np.array(f[dg_ref]['data']),\n",
    "        np.array(f[dg_ref]['ir']), np.array(f[dg_ref]['jc'])),\n",
    "        shape=(12331, row_list[i]))\n",
    "    gene_feature_list_other_spe.append(disease_gene_adj_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26646523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "disease_tfidf_path = './Disease_gene_prioritization_GCN/data_prioritization/clinicalfeatures_tfidf.mat'\n",
    "f_disease_tfidf = h5py.File(disease_tfidf_path)\n",
    "disease_tfidf = np.array(f_disease_tfidf['F'])\n",
    "disease_tfidf = np.transpose(disease_tfidf)\n",
    "disease_tfidf = sp.csc_matrix(disease_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67482211",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_dis_adj_list= list()\n",
    "dis_dis_adj_list.append(disease_network_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6657a5",
   "metadata": {},
   "source": [
    "# DATA PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b13a917f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12331,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_test_size = 0.1\n",
    "n_genes = 12331\n",
    "n_dis = 3215\n",
    "n_dis_rel_types = len(dis_dis_adj_list)\n",
    "gene_adj = gene_network_adj\n",
    "gene_degrees = np.array(gene_adj.sum(axis=0)).squeeze()\n",
    "gene_degrees.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c75d1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3215,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_dis_adj = gene_disease_adj\n",
    "dis_gene_adj = gene_dis_adj.transpose(copy=True)\n",
    "dis_degrees_list = [np.array(dis_adj.sum(axis=0)).squeeze() for dis_adj in dis_dis_adj_list]\n",
    "dis_degrees_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96af076f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adj_mats_orig = {\n",
    "    (0, 0): [gene_adj, gene_adj.transpose(copy=True)],\n",
    "    (0, 1): [gene_dis_adj],\n",
    "    (1, 0): [dis_gene_adj],\n",
    "    (1, 1): dis_dis_adj_list + [x.transpose(copy=True) for x in dis_dis_adj_list],\n",
    "}\n",
    "degrees = {\n",
    "    0: [gene_degrees, gene_degrees],\n",
    "    1: dis_degrees_list + dis_degrees_list,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "000198df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gene Feature Exp - Feature vector  + other 8 species\n",
    "gene_feat = sp.hstack(gene_feature_list_other_spe+[gene_feature_exp])\n",
    "gene_nonzero_feat, gene_num_feat = gene_feat.shape\n",
    "gene_feat = sparse_to_tuple(gene_feat.tocoo())\n",
    "\n",
    "dis_feat = disease_tfidf\n",
    "dis_nonzero_feat, dis_num_feat = dis_feat.shape\n",
    "dis_feat = sparse_to_tuple(dis_feat.tocoo())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5265eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feat = {\n",
    "    0: gene_num_feat,\n",
    "    1: dis_num_feat,\n",
    "}\n",
    "nonzero_feat = {\n",
    "    0: gene_nonzero_feat,\n",
    "    1: dis_nonzero_feat,\n",
    "}\n",
    "feat = {\n",
    "    0: gene_feat,\n",
    "    1: dis_feat,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "933d2589",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_type2dim = {k: [adj.shape for adj in adjs] for k, adjs in adj_mats_orig.items()}\n",
    "# edge_type2decoder = {\n",
    "#     (0, 0): 'bilinear',\n",
    "#     (0, 1): 'bilinear',\n",
    "#     (1, 0): 'bilinear',\n",
    "#     (1, 1): 'bilinear',\n",
    "# }\n",
    "\n",
    "edge_type2decoder = {\n",
    "    (0, 0): 'innerproduct',\n",
    "    (0, 1): 'innerproduct',\n",
    "    (1, 0): 'innerproduct',\n",
    "    (1, 1): 'innerproduct',\n",
    "}\n",
    "\n",
    "edge_types = {k: len(v) for k, v in adj_mats_orig.items()}\n",
    "num_edge_types = sum(edge_types.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbc5c07",
   "metadata": {},
   "source": [
    "# INSIDE MAIN CALLING CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72b24282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "del_all_flags(tf.compat.v1.app.flags.FLAGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603abbc6",
   "metadata": {},
   "source": [
    "# FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fc5a7ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_sample_size =  1  #Negative sample size\n",
    "learning_rate =  0.01 # 'Initial learning rate\n",
    "hidden1_units =  64 #'Number of units in hidden layer 1\n",
    "hidden2_units = 32 #Number of units in hidden layer 2\n",
    "weight_decay =  0.001  #Weight for L2 loss on embedding matrix\n",
    "dropout = 0.1 # Dropout rate (1 - keep probability)\n",
    "max_margin =  0.1 #Max margin parameter in hinge loss\n",
    "batch_size = 512\n",
    "bias = True #Bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc39102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_placeholders(edge_types):\n",
    "    placeholders = {\n",
    "        'batch': tf.compat.v1.placeholder(tf.int32, name='batch'),\n",
    "        'batch_neg': tf.compat.v1.placeholder(tf.int32, name='batch_neg'),\n",
    "        'batch_edge_type_idx': tf.compat.v1.placeholder(tf.int32, shape=(), name='batch_edge_type_idx'),\n",
    "        'batch_row_edge_type': tf.compat.v1.placeholder(tf.int32, shape=(), name='batch_row_edge_type'),\n",
    "        'batch_col_edge_type': tf.compat.v1.placeholder(tf.int32, shape=(), name='batch_col_edge_type'),\n",
    "        'degrees': tf.compat.v1.placeholder(tf.int32),\n",
    "        'dropout': tf.compat.v1.placeholder_with_default(0., shape=()),\n",
    "    }\n",
    "    placeholders.update({\n",
    "        'adj_mats_%d,%d,%d' % (i, j, k): tf.compat.v1.sparse_placeholder(tf.float32)\n",
    "        for i, j in edge_types for k in range(edge_types[i,j])})\n",
    "    placeholders.update({\n",
    "        'feat_%d' % i: tf.compat.v1.sparse_placeholder(tf.float32)\n",
    "        for i, _ in edge_types})\n",
    "    return placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5678f65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining placeholders\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining placeholders\")\n",
    "placeholders = construct_placeholders(edge_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e51508",
   "metadata": {},
   "source": [
    "# Defining Minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cfeb741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeMinibatchIterator(object):\n",
    "    \"\"\" This minibatch iterator iterates over batches of sampled edges or\n",
    "    random pairs of co-occuring edges.\n",
    "    assoc -- numpy array with target edges\n",
    "    placeholders -- tensorflow placeholders object\n",
    "    batch_size -- size of the minibatches\n",
    "    \"\"\"\n",
    "    def __init__(self, adj_mats, feat, edge_types, batch_size=100, val_test_size=0.01):\n",
    "        self.adj_mats = adj_mats  #Cell 26\n",
    "        self.feat = feat           #{ 0: [12331, 17480 feature matrix], 1: [3215, 16592 feature matrix] }\n",
    "        self.edge_types = edge_types     #{(0, 0): 2, (0, 1): 1, (1, 0): 1, (1, 1): 2}\n",
    "        self.batch_size = batch_size         #512\n",
    "        self.val_test_size = val_test_size       #0.1\n",
    "        self.num_edge_types = sum(self.edge_types.values())        #6\n",
    "\n",
    "        self.iter = 0\n",
    "        self.freebatch_edge_types= list(range(self.num_edge_types))            #{[0,1,2,3,4,5]}\n",
    "        self.batch_num = [0]*self.num_edge_types        #[0,0,0,0,0,0]\n",
    "        self.current_edge_type_idx = 0      \n",
    "        self.edge_type2idx = {}\n",
    "        self.idx2edge_type = {}\n",
    "        r = 0\n",
    "        for i, j in self.edge_types:\n",
    "            for k in range(self.edge_types[i,j]):\n",
    "                self.edge_type2idx[i, j, k] = r          #{(0,0,0): 0, (0,0,1): 1 ...}\n",
    "                self.idx2edge_type[r] = i, j, k          #{0: (0,0,0), 1: (0,0,1) ...}\n",
    "                r += 1\n",
    "\n",
    "        self.train_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()} #{(0,0):[None, None], ..}\n",
    "        self.val_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
    "        self.test_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
    "        self.test_edges_false = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
    "        self.val_edges_false = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
    "\n",
    "        # Function to build test and val sets with val_test_size positive links\n",
    "        self.adj_train = {edge_type: [None]*n for edge_type, n in self.edge_types.items()} #{(0,0):[None, None], ..}\n",
    "        for i, j in self.edge_types:\n",
    "            for k in range(self.edge_types[i,j]):\n",
    "                self.mask_test_edges((i, j), k)\n",
    "                \n",
    "    def mask_test_edges(self, edge_type, type_idx):\n",
    "        edges_all, _, _ = sparse_to_tuple(self.adj_mats[edge_type][type_idx])  #Cell 26 convert to {(n*2: x,y),n*val,shape} Cell 40\n",
    "        num_test = max(100, int(np.floor(edges_all.shape[0] * self.val_test_size)))  #edges_all are the coords in adj matrix\n",
    "        num_val = max(100, int(np.floor(edges_all.shape[0] * self.val_test_size)))   #num_test is max 100 ??? WHYYYY??\n",
    "\n",
    "        if edge_type not in [(0,1), (1, 0)]:   # DIS_DIS and GENE_GENE only 10 ???? WHYY??\n",
    "            num_test = 10\n",
    "            num_val = 10\n",
    "\n",
    "        all_edge_idx = list(range(edges_all.shape[0]))  #edges_all.shape[0] - > Num of non_zero values in sparse [0,1 .. N]\n",
    "        np.random.shuffle(all_edge_idx)\n",
    "\n",
    "        val_edge_idx = all_edge_idx[:num_val]\n",
    "        val_edges = edges_all[val_edge_idx]\n",
    "\n",
    "        test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
    "        test_edges = edges_all[test_edge_idx]\n",
    "\n",
    "        train_edges = np.delete(edges_all, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
    "\n",
    "        #Checks if edges are part of edges_all, then adds to test_edges_false if not there, same length as test_edges\n",
    "        test_edges_false = []\n",
    "        while len(test_edges_false) < len(test_edges):\n",
    "            idx_i = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[0])\n",
    "            idx_j = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[1])\n",
    "            if self._ismember([idx_i, idx_j], edges_all):\n",
    "                continue\n",
    "            if test_edges_false:\n",
    "                if self._ismember([idx_i, idx_j], test_edges_false):\n",
    "                    continue\n",
    "            test_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "        val_edges_false = []\n",
    "        while len(val_edges_false) < len(val_edges):\n",
    "            idx_i = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[0])\n",
    "            idx_j = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[1])\n",
    "            if self._ismember([idx_i, idx_j], edges_all):\n",
    "                continue\n",
    "            if val_edges_false:\n",
    "                if self._ismember([idx_i, idx_j], val_edges_false):\n",
    "                    continue\n",
    "            val_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "        #ALL _edges and _edges_false are arrays of coordinates [n*[x,y]]\n",
    "        # Re-build adj matrices\n",
    "        data = np.ones(train_edges.shape[0])\n",
    "        adj_train = sp.csr_matrix(\n",
    "            (data, (train_edges[:, 0], train_edges[:, 1])), \n",
    "            shape=self.adj_mats[edge_type][type_idx].shape)        #adj_train are now just 1's\n",
    "        self.adj_train[edge_type][type_idx] = self.preprocess_graph(adj_train)\n",
    "\n",
    "        self.train_edges[edge_type][type_idx] = train_edges\n",
    "        self.val_edges[edge_type][type_idx] = val_edges\n",
    "        self.val_edges_false[edge_type][type_idx] = np.array(val_edges_false)\n",
    "        self.test_edges[edge_type][type_idx] = test_edges\n",
    "        self.test_edges_false[edge_type][type_idx] = np.array(test_edges_false)\n",
    "        \n",
    "    def _ismember(self, a, b):\n",
    "        a = np.array(a)\n",
    "        b = np.array(b)\n",
    "        rows_close = np.all(a - b == 0, axis=1)  #Subtracts the pair a(idx_i, idx_j) from each element of b, if equal then \n",
    "        return np.any(rows_close)                #Function will return true if found a coordinates\n",
    "     \n",
    "    # Symteric (D^-0.5 . (A + I) . D^-0.5) and Assymteric (Dr ^ -0.5 . A . Dc ^ -0.5) normalization\n",
    "    # For A(mxm) -> adj_ = A + I(mxm), R(mx1) = Sum of rows of adj_, M_inv = D(mxm) (diaginals are roots of R), \n",
    "    # Adj_Normalized = (adj(mxm).D(mxm))'.D(mxm)\n",
    "    #For A(mxn) -> R(mx1) = sum of rows of A, C(nx1) = sum of cols of A, RD(mxm) -> Diagonals(non-null) are roots of R,\n",
    "    # CD(nxn) -> Diagonals(non-null) are roots of C  \n",
    "    # Adj_normalized = RD(mxm).A(mxn).CD(nxn) \n",
    "    def preprocess_graph(self, adj):\n",
    "        adj = sp.coo_matrix(adj)\n",
    "        if adj.shape[0] == adj.shape[1]:\n",
    "            adj_ = adj + sp.eye(adj.shape[0])\n",
    "            rowsum = np.array(adj_.sum(1))\n",
    "            degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "            adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "        else:\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "            colsum = np.array(adj.sum(0))\n",
    "            rowdegree_mat_inv = sp.diags(np.nan_to_num(np.power(rowsum, -0.5)).flatten())\n",
    "            coldegree_mat_inv = sp.diags(np.nan_to_num(np.power(colsum, -0.5)).flatten())\n",
    "            adj_normalized = rowdegree_mat_inv.dot(adj).dot(coldegree_mat_inv).tocoo()\n",
    "        return sparse_to_tuple(adj_normalized)\n",
    "    \n",
    "    def update_feed_dict(self, feed_dict, dropout, placeholders):\n",
    "        # construct feed dictionary\n",
    "        feed_dict.update({\n",
    "            placeholders['adj_mats_%d,%d,%d' % (i,j,k)]: self.adj_train[i,j][k]\n",
    "            for i, j in self.edge_types for k in range(self.edge_types[i,j])})\n",
    "        feed_dict.update({placeholders['feat_%d' % i]: self.feat[i] for i, _ in self.edge_types})\n",
    "        feed_dict.update({placeholders['dropout']: dropout})\n",
    "        return feed_dict\n",
    "    \n",
    "    def batch_feed_dict(self, batch_edges, batch_edges_neg, batch_edge_type, placeholders):\n",
    "        feed_dict = dict()\n",
    "        feed_dict.update({placeholders['batch']: batch_edges})   # List of [x,y] coordinates\n",
    "        feed_dict.update({placeholders['batch_neg']: batch_edges_neg})\n",
    "        feed_dict.update({placeholders['batch_edge_type_idx']: batch_edge_type}) # Type of edge, {0: (0,0,0), 1: (0,0,1) ..}\n",
    "        feed_dict.update({placeholders['batch_row_edge_type']: self.idx2edge_type[batch_edge_type][0]}) # From Edge\n",
    "        feed_dict.update({placeholders['batch_col_edge_type']: self.idx2edge_type[batch_edge_type][1]}) # To Edge\n",
    "        return feed_dict\n",
    "    \n",
    "    def create_negative_batch(self, batch_edges):\n",
    "        batch_edges_false = []\n",
    "        i, j, k = self.idx2edge_type[self.current_edge_type_idx]\n",
    "        for idx_j in batch_edges[:,1]: \n",
    "            while True:\n",
    "                idx_i = np.random.randint(0, self.adj_mats[i,j][k].shape[0])\n",
    "                if self._ismember([idx_i, idx_j], self.train_edges[i,j][k]):\n",
    "                    continue\n",
    "                if batch_edges_false:\n",
    "                    if self._ismember([idx_i, idx_j], batch_edges_false):\n",
    "                        continue\n",
    "                #assert self.adj_mats[i,j][k][idx_i,idx_j] == 0\n",
    "                batch_edges_false.append([idx_i, idx_j])\n",
    "                break\n",
    "        batch_edges_false = np.array(batch_edges_false)\n",
    "        assert batch_edges_false.shape == (self.batch_size,2)\n",
    "        return batch_edges_false\n",
    "            \n",
    "    def next_minibatch_feed_dict(self, placeholders):\n",
    "        \"\"\"Select a random edge type and a batch of edges of the same type\"\"\"\n",
    "        print(\"Current Iteration: \", self.iter)\n",
    "        self.current_edge_type_idx = self.iter%len(minibatch.idx2edge_type)\n",
    "        print(\"Current Edge: \", self.current_edge_type_idx )\n",
    "        self.iter += 1\n",
    "        \n",
    "        i, j, k = self.idx2edge_type[self.current_edge_type_idx]\n",
    "        if self.batch_num[self.current_edge_type_idx] * self.batch_size \\\n",
    "                   > len(self.train_edges[i,j][k]) - self.batch_size + 1:   #Less than batch size elements left\n",
    "            batch_edges = self.train_edges[i,j][k][self.batch_num[self.current_edge_type_idx] * self.batch_size:] \n",
    "            print(\"Extracting train_edges[\", str(i), \",\",str(j),\"][\",self.batch_num[self.current_edge_type_idx] * self.batch_size,\":]\")\n",
    "            if(batch_edges.shape[0] < 512):\n",
    "                batch_edges = np.concatenate([batch_edges, self.train_edges[i,j][k][0:self.batch_size - batch_edges.shape[0]]], axis = 0)\n",
    "                print(\"Extracting train_edges[\", str(i), \",\",str(j),\"][0:\",self.batch_size - batch_edges.shape[0],\"]\")\n",
    "            else:\n",
    "                batch_edges = batch_edges[0:self.batch_size]\n",
    "            self.batch_num[self.current_edge_type_idx] = 0\n",
    "            assert batch_edges.shape == (self.batch_size,2)\n",
    "            batch_edges_neg = self.create_negative_batch(batch_edges)\n",
    "            return self.batch_feed_dict(batch_edges, batch_edges_neg, self.current_edge_type_idx, placeholders)\n",
    "\n",
    "\n",
    "        \n",
    "        start = self.batch_num[self.current_edge_type_idx] * self.batch_size\n",
    "        self.batch_num[self.current_edge_type_idx] += 1\n",
    "        print(\"Extracting train_edges[\", str(i), \",\",str(j),\"][\", str(k),\"][\", start, \":\", start + self.batch_size,\"]\")\n",
    "        batch_edges = self.train_edges[i,j][k][start: start + self.batch_size]\n",
    "        assert batch_edges.shape == (self.batch_size,2)\n",
    "        batch_edges_neg = self.create_negative_batch(batch_edges)\n",
    "        return self.batch_feed_dict(batch_edges,batch_edges_neg, self.current_edge_type_idx, placeholders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fca60fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function concatenate at 0x7f876c479b30>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5388e096",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1469132/1778063838.py:120: RuntimeWarning: divide by zero encountered in power\n",
      "  rowdegree_mat_inv = sp.diags(np.nan_to_num(np.power(rowsum, -0.5)).flatten())\n",
      "/tmp/ipykernel_1469132/1778063838.py:121: RuntimeWarning: divide by zero encountered in power\n",
      "  coldegree_mat_inv = sp.diags(np.nan_to_num(np.power(colsum, -0.5)).flatten())\n"
     ]
    }
   ],
   "source": [
    "minibatch = EdgeMinibatchIterator(\n",
    "        adj_mats=adj_mats_orig,  #Check cell 26\n",
    "        feat=feat,              #{ 0: [12331, 17480 feature matrix], 1: [3215, 16592 feature matrix] }\n",
    "        edge_types=edge_types,   #{(0, 0): 2, (0, 1): 1, (1, 0): 1, (1, 1): 2}\n",
    "        batch_size=batch_size,      #512\n",
    "        val_test_size=val_test_size  #0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22197b58",
   "metadata": {},
   "source": [
    "##  Defining The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03006503",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
    "    \"\"\"Create a weight variable with Glorot & Bengio (AISTATS 2010)\n",
    "    initialization.\n",
    "    \"\"\"\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = tf.compat.v1.random_uniform([input_dim, output_dim], minval=-init_range,\n",
    "                                maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5907434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs\n",
    "    \"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "\n",
    "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
    "    \"\"\"Dropout for sparse tensors. Currently fails for very large sparse tensors (>1M elements) # TODO: CHANGE THIS\n",
    "    \"\"\"\n",
    "    noise_shape = [num_nonzero_elems]\n",
    "    random_tensor = keep_prob         # 1 - dropout\n",
    "    random_tensor += tf.compat.v1.random_uniform(noise_shape)     #Add 1 - dropout to random matrix of Nx1, N is input \n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)    # If > 1, keep or else drop\n",
    "    pre_out = tf.compat.v1.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1./keep_prob)                             # The Remaining elements get multiplied, dropout concept\n",
    "\n",
    "\n",
    "class MultiLayer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "\n",
    "    # Properties    \n",
    "        name: String, defines the variable scope of the layer.\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_type=(), num_types=-1, **kwargs):\n",
    "        self.edge_type = edge_type\n",
    "        self.num_types = num_types\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "        self.issparse = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            outputs = self._call(inputs)\n",
    "            return outputs\n",
    "\n",
    "\n",
    "class GraphConvolutionSparseMulti(MultiLayer):\n",
    "    \"\"\"Graph convolution layer for sparse inputs.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, adj_mats,\n",
    "                 nonzero_feat, dropout=0., act=tf.nn.relu, **kwargs):\n",
    "        super(GraphConvolutionSparseMulti, self).__init__(**kwargs) #input_dim = num_feat:  {0: 17480, 1: 16592}\n",
    "        self.dropout = dropout\n",
    "        self.adj_mats = adj_mats\n",
    "        self.act = act\n",
    "        self.issparse = True\n",
    "        self.nonzero_feat = nonzero_feat  #nonzero_feat:  {0: 12331, 1: 3215}\n",
    "        with tf.compat.v1.variable_scope('%s_vars' % self.name):      #weights_i are glorot variables: input_dimxoutput_dim matrix\n",
    "            for k in range(self.num_types):\n",
    "                print(\"Initializing weights: weights_%d with shape: \" %k)\n",
    "                print(str(input_dim[self.edge_type[1]]) + \" \" + str(output_dim))\n",
    "                self.vars['weights_%d' % k] = weight_variable_glorot(\n",
    "                    input_dim[self.edge_type[1]], output_dim, name='weights_%d' % k) #input_dim:  {0: 17480, 1: 16592}\n",
    "                                                                                    #output_dim: hidden_1 units = 64\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        outputs = []\n",
    "        for k in range(self.num_types):\n",
    "            x = dropout_sparse(inputs, 1-self.dropout, self.nonzero_feat[self.edge_type[1]]) # x is shape 12331(3215) \n",
    "            x = tf.compat.v1.sparse_tensor_dense_matmul(x, self.vars['weights_%d' % k])       # x 12331 * []\n",
    "            x = tf.compat.v1.sparse_tensor_dense_matmul(self.adj_mats[self.edge_type][k], x)\n",
    "            outputs.append(self.act(x))\n",
    "        outputs = tf.add_n(outputs)\n",
    "        outputs = tf.nn.l2_normalize(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class GraphConvolutionMulti(MultiLayer):\n",
    "    \"\"\"Basic graph convolution layer for undirected graph without edge labels.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, adj_mats, dropout=0., act=tf.nn.relu, **kwargs):\n",
    "        super(GraphConvolutionMulti, self).__init__(**kwargs)\n",
    "        self.adj_mats = adj_mats\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        with tf.compat.v1.variable_scope('%s_vars' % self.name):\n",
    "            for k in range(self.num_types):\n",
    "                self.vars['weights_%d' % k] = weight_variable_glorot(\n",
    "                    input_dim, output_dim, name='weights_%d' % k)\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        outputs = []\n",
    "        for k in range(self.num_types):\n",
    "            x = tf.nn.dropout(inputs, 1-self.dropout)\n",
    "            x = tf.matmul(x, self.vars['weights_%d' % k])\n",
    "            x = tf.compat.v1.sparse_tensor_dense_matmul(self.adj_mats[self.edge_type][k], x)\n",
    "            outputs.append(self.act(x))\n",
    "        outputs = tf.add_n(outputs)\n",
    "        outputs = tf.nn.l2_normalize(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InnerProductDecoder(MultiLayer):\n",
    "    \"\"\"Decoder model layer for link prediction.\"\"\"\n",
    "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
    "        super(InnerProductDecoder, self).__init__(**kwargs)\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        i, j = self.edge_type\n",
    "        outputs = []\n",
    "        for k in range(self.num_types):\n",
    "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
    "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
    "            rec = tf.matmul(inputs_row, tf.transpose(inputs_col))\n",
    "            outputs.append(self.act(rec))\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c9e1e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', True)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.compat.v1.variable_scope(self.name):\n",
    "            self._build()\n",
    "        variables = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class DecagonModel(Model):\n",
    "    def __init__(self, placeholders, num_feat, nonzero_feat, edge_types, decoders, **kwargs):\n",
    "        super(DecagonModel, self).__init__(**kwargs)\n",
    "        self.edge_types = edge_types    #{(0, 0): 2, (0, 1): 1, (1, 0): 1, (1, 1): 2}\n",
    "        self.num_edge_types = sum(self.edge_types.values())      #6\n",
    "        self.num_obj_types = max([i for i, _ in self.edge_types]) + 1       #2\n",
    "        self.decoders = decoders #{(0, 0): 'innerproduct', (0, 1): 'innerproduct', (1, 0): 'innerproduct', (1, 1): 'innerproduct'}    \n",
    "        self.inputs = {i: placeholders['feat_%d' % i] for i, _ in self.edge_types} #sparse tensors with n*indices, n*values, shape\n",
    "        self.input_dim = num_feat  #num_feat:  {0: 17480, 1: 16592}\n",
    "        self.nonzero_feat = nonzero_feat #nonzero_feat:  {0: 12331, 1: 3215}\n",
    "        self.placeholders = placeholders\n",
    "        self.dropout = placeholders['dropout']  #default 0\n",
    "        self.adj_mats = {et: [\n",
    "            placeholders['adj_mats_%d,%d,%d' % (et[0], et[1], k)] for k in range(n)]\n",
    "            for et, n in self.edge_types.items()}\n",
    "        self.build()\n",
    "\n",
    "    def _build(self):\n",
    "        self.hidden1 = defaultdict(list)  #Create empty list when trying to access key not there\n",
    "        for i, j in self.edge_types:\n",
    "            self.hidden1[i].append(GraphConvolutionSparseMulti(\n",
    "                input_dim=self.input_dim, output_dim=hidden1_units,\n",
    "                edge_type=(i,j), num_types=self.edge_types[i,j],         \n",
    "                adj_mats=self.adj_mats, nonzero_feat=self.nonzero_feat,\n",
    "                act=lambda x: x, dropout=self.dropout,       # no activation ??? \n",
    "                logging=self.logging)(self.inputs[j]))      # Logging is only true or false\n",
    "            # self.hidden1[i].append(GraphConvolutionMulti(\n",
    "            #     input_dim=self.input_dim, output_dim=hidden1_units,\n",
    "            #     edge_type=(i,j), num_types=self.edge_types[i,j],\n",
    "            #     adj_mats=self.adj_mats, act=lambda x: x, \n",
    "            #     dropout=self.dropout,logging=self.logging)(self.inputs[j]))\n",
    "\n",
    "        for i, hid1 in self.hidden1.items():\n",
    "            self.hidden1[i] = tf.nn.relu(tf.add_n(hid1))\n",
    "\n",
    "        self.embeddings_reltyp = defaultdict(list)\n",
    "        for i, j in self.edge_types:\n",
    "            self.embeddings_reltyp[i].append(GraphConvolutionMulti(\n",
    "                input_dim=hidden1_units, output_dim=hidden2_units,\n",
    "                edge_type=(i,j), num_types=self.edge_types[i,j],\n",
    "                adj_mats=self.adj_mats, act=lambda x: x,\n",
    "                dropout=self.dropout, logging=self.logging)(self.hidden1[j]))\n",
    "\n",
    "        self.embeddings = [None] * self.num_obj_types\n",
    "        for i, embeds in self.embeddings_reltyp.items():\n",
    "            # self.embeddings[i] = tf.nn.relu(tf.add_n(embeds))\n",
    "            self.embeddings[i] = tf.add_n(embeds)\n",
    "\n",
    "        self.edge_type2decoder = {}\n",
    "        for i, j in self.edge_types:\n",
    "            decoder = self.decoders[i, j]\n",
    "            if decoder == 'innerproduct':\n",
    "                self.edge_type2decoder[i, j] = InnerProductDecoder(\n",
    "                    input_dim=hidden2_units, logging=self.logging,\n",
    "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
    "                    act=lambda x: x, dropout=self.dropout)\n",
    "            else:\n",
    "                raise ValueError('Unknown decoder type')\n",
    "\n",
    "        self.latent_inters = []\n",
    "        self.latent_varies = []\n",
    "        for edge_type in self.edge_types:\n",
    "            decoder = self.decoders[edge_type]\n",
    "            for k in range(self.edge_types[edge_type]):\n",
    "                if decoder == 'innerproduct':\n",
    "                    glb = tf.eye(hidden2_units, hidden2_units)\n",
    "                    loc = tf.eye(hidden2_units, hidden2_units)\n",
    "                else:\n",
    "                    raise ValueError('Unknown decoder type')\n",
    "\n",
    "                self.latent_inters.append(glb)\n",
    "                self.latent_varies.append(loc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49c6a5d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create model\n",
      "Initializing weights: weights_0 with shape: \n",
      "17480 64\n",
      "Initializing weights: weights_1 with shape: \n",
      "17480 64\n",
      "WARNING:tensorflow:From /home/indradutta/miniconda3/envs/GNNGeneDisease/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "Initializing weights: weights_0 with shape: \n",
      "16592 64\n",
      "Initializing weights: weights_0 with shape: \n",
      "17480 64\n",
      "Initializing weights: weights_0 with shape: \n",
      "16592 64\n",
      "Initializing weights: weights_1 with shape: \n",
      "16592 64\n"
     ]
    }
   ],
   "source": [
    "print(\"Create model\")\n",
    "model = DecagonModel(\n",
    "        placeholders=placeholders,\n",
    "        num_feat=num_feat,\n",
    "        nonzero_feat=nonzero_feat,\n",
    "        edge_types=edge_types,\n",
    "        decoders=edge_type2decoder,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fe3c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecagonOptimizer(object):\n",
    "    def __init__(self, embeddings, latent_inters, latent_varies,\n",
    "                 degrees, edge_types, edge_type2dim, placeholders,\n",
    "                 margin=0.1, neg_sample_weights=1., batch_size=100):\n",
    "        self.embeddings= embeddings\n",
    "        self.latent_inters = latent_inters\n",
    "        self.latent_varies = latent_varies\n",
    "        self.edge_types = edge_types\n",
    "        self.degrees = degrees\n",
    "        self.edge_type2dim = edge_type2dim           #(0, 0): [(12331, 12331), (12331, 12331)] ... \n",
    "        self.obj_type2n = {i: self.edge_type2dim[i,j][0][0] for i, j in self.edge_types}  #{0: 12331, 1: 3215}\n",
    "        self.margin = margin\n",
    "        self.neg_sample_weights = neg_sample_weights\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.inputs = placeholders['batch']\n",
    "        self.batch_edge_type_idx = placeholders['batch_edge_type_idx']\n",
    "        self.batch_row_edge_type = placeholders['batch_row_edge_type']\n",
    "        self.batch_col_edge_type = placeholders['batch_col_edge_type']\n",
    "\n",
    "        self.row_inputs = tf.squeeze(gather_cols(self.inputs, [0]))\n",
    "        self.col_inputs = tf.squeeze(gather_cols(self.inputs, [1]))\n",
    "\n",
    "        obj_type_n = [self.obj_type2n[i] for i in range(len(self.embeddings))]  \n",
    "        self.obj_type_lookup_start = tf.cumsum([0] + obj_type_n[:-1])    #0, 12331\n",
    "        self.obj_type_lookup_end = tf.cumsum(obj_type_n)                 #12331, 12331 + 3215\n",
    "\n",
    "        labels = tf.reshape(tf.cast(self.row_inputs, dtype=tf.int64), [self.batch_size, 1])\n",
    "        \n",
    "        self.neg_samples_list = []\n",
    "        for i, j in self.edge_types:\n",
    "            for k in range(self.edge_types[i,j]):\n",
    "                print(\"Creating Neg Sampes with range max: \", len(self.degrees[i][k]), \"for\", i, j, k)\n",
    "                print_op = tf.print(\"For Neg Samples: Labels: \", labels, \"batch_index: \", self.batch_edge_type_idx,\"range_max: \",len(self.degrees[i][k]))\n",
    "                with tf.compat.v1.control_dependencies([print_op]):\n",
    "                    neg_samples, _, _ = tf.nn.fixed_unigram_candidate_sampler(\n",
    "                        true_classes=labels,\n",
    "                        num_true=1,\n",
    "                        num_sampled=self.batch_size,\n",
    "                        unique=False,\n",
    "                        range_max=len(self.degrees[i][k]),\n",
    "                        distortion=0.75,\n",
    "                        unigrams=self.degrees[i][k].tolist())\n",
    "                self.neg_samples_list.append(neg_samples)\n",
    "                \n",
    "        #self.neg_samples = tf.gather(self.neg_samples_list, self.batch_edge_type_idx)\n",
    "        self.neg_samples = placeholders['batch_neg']\n",
    "        self.neg_samples_rows = tf.squeeze(gather_cols(self.neg_samples, [0]))\n",
    "        \n",
    "        self.preds = self.batch_predict(self.row_inputs, self.col_inputs)   # Diagonol has the product of the row-col embeddings\n",
    "        self.outputs = tf.compat.v1.diag_part(self.preds)\n",
    "        self.outputs = tf.reshape(self.outputs, [-1])         \n",
    "\n",
    "        self.neg_preds = self.batch_predict(self.neg_samples_rows, self.col_inputs)\n",
    "        self.neg_outputs = tf.compat.v1.diag_part(self.neg_preds)\n",
    "        self.neg_outputs = tf.reshape(self.neg_outputs, [-1])\n",
    "\n",
    "        self.predict()\n",
    "\n",
    "        self._build()\n",
    "\n",
    "    def batch_predict(self, row_inputs, col_inputs):\n",
    "        concatenated = tf.concat(self.embeddings, 0)  # 12331x32 + 3215x32 together along 0 axis\n",
    "\n",
    "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_row_edge_type)\n",
    "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_row_edge_type)\n",
    "        indices = tf.range(ind_start, ind_end)\n",
    "        row_embeds = tf.gather(concatenated, indices)            # Only gather embeddings for the partcular type, 1st 12331, or next 3215\n",
    "        row_embeds = tf.gather(row_embeds, row_inputs)         # Takes the 512 embeddings in current batch\n",
    "\n",
    "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_col_edge_type) # Gather embeddings for Col type\n",
    "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_col_edge_type)\n",
    "        indices = tf.range(ind_start, ind_end)\n",
    "        col_embeds = tf.gather(concatenated, indices)\n",
    "        col_embeds = tf.gather(col_embeds, col_inputs)        # Takes the 512 embeddings in current batch\n",
    "\n",
    "        latent_inter = tf.gather(self.latent_inters, self.batch_edge_type_idx)  # Identity matrix 32x32\n",
    "        latent_var = tf.gather(self.latent_varies, self.batch_edge_type_idx)     # Identity matrix 32x32\n",
    "\n",
    "        product1 = tf.matmul(row_embeds, latent_var)   # No effect in Innerproduct for all 3 products\n",
    "        product2 = tf.matmul(product1, latent_inter)\n",
    "        product3 = tf.matmul(product2, latent_var)\n",
    "        preds = tf.matmul(product3, tf.transpose(col_embeds))  #512x32 * 32*512\n",
    "        return preds          # Predition is simply product of two embeddings\n",
    "\n",
    "    def predict(self):\n",
    "        concatenated = tf.concat(self.embeddings, 0)\n",
    "\n",
    "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_row_edge_type)\n",
    "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_row_edge_type)\n",
    "        indices = tf.range(ind_start, ind_end)\n",
    "        row_embeds = tf.gather(concatenated, indices)\n",
    "\n",
    "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_col_edge_type)\n",
    "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_col_edge_type)\n",
    "        indices = tf.range(ind_start, ind_end)\n",
    "        col_embeds = tf.gather(concatenated, indices)\n",
    "\n",
    "        latent_inter = tf.gather(self.latent_inters, self.batch_edge_type_idx)\n",
    "        latent_var = tf.gather(self.latent_varies, self.batch_edge_type_idx)\n",
    "\n",
    "        product1 = tf.matmul(row_embeds, latent_var)\n",
    "        product2 = tf.matmul(product1, latent_inter)\n",
    "        product3 = tf.matmul(product2, latent_var)\n",
    "        self.predictions = tf.matmul(product3, tf.transpose(col_embeds))    # Predition is simply product of two embeddings\n",
    "\n",
    "    def _build(self):\n",
    "        #self.cost = self._hinge_loss(self.outputs, self.neg_outputs)\n",
    "        self.cost = self._xent_loss(self.outputs, self.neg_outputs)\n",
    "        self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.cost)\n",
    "        self.grads_vars = self.optimizer.compute_gradients(self.cost)\n",
    "\n",
    "    def _hinge_loss(self, aff, neg_aff):\n",
    "        \"\"\"Maximum-margin optimization using the hinge loss.\"\"\"\n",
    "        diff = tf.nn.relu(tf.subtract(neg_aff, tf.expand_dims(aff, 0) - self.margin), name='diff')\n",
    "        loss = tf.reduce_sum(diff)\n",
    "        return loss\n",
    "\n",
    "    def _xent_loss(self, aff, neg_aff):\n",
    "        \"\"\"Cross-entropy optimization.\"\"\"\n",
    "        true_xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(aff), logits=aff)\n",
    "        negative_xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(neg_aff), logits=neg_aff)\n",
    "        loss = tf.reduce_sum(true_xent) + self.neg_sample_weights * tf.reduce_sum(negative_xent)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def gather_cols(params, indices, name=None):\n",
    "    \"\"\"Gather columns of a 2D tensor.\n",
    "\n",
    "    Args:\n",
    "        params: A 2D tensor.\n",
    "        indices: A 1D tensor. Must be one of the following types: ``int32``, ``int64``.\n",
    "        name: A name for the operation (optional).\n",
    "\n",
    "    Returns:\n",
    "        A 2D Tensor. Has the same type as ``params``.\n",
    "    \"\"\"\n",
    "    with tf.compat.v1.op_scope([params, indices], name, \"gather_cols\") as scope:\n",
    "        # Check input\n",
    "        params = tf.convert_to_tensor(params, name=\"params\")\n",
    "        indices = tf.convert_to_tensor(indices, name=\"indices\")\n",
    "        try:\n",
    "            params.get_shape().assert_has_rank(2)\n",
    "        except ValueError:\n",
    "            raise ValueError('\\'params\\' must be 2D.')\n",
    "        try:\n",
    "            indices.get_shape().assert_has_rank(1)\n",
    "        except ValueError:\n",
    "            raise ValueError('\\'params\\' must be 1D.')\n",
    "\n",
    "        # Define op\n",
    "        p_shape = tf.shape(params)\n",
    "        p_flat = tf.reshape(params, [-1])\n",
    "        i_flat = tf.reshape(tf.reshape(tf.range(0, p_shape[0]) * p_shape[1],\n",
    "                                       [-1, 1]) + indices, [-1])\n",
    "        return tf.reshape(\n",
    "            tf.gather(p_flat, i_flat), [p_shape[0], -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45657796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create optimizer\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "Creating Neg Sampes with range max:  12331 for 0 0 0\n",
      "Creating Neg Sampes with range max:  12331 for 0 0 1\n",
      "Creating Neg Sampes with range max:  12331 for 0 1 0\n",
      "Creating Neg Sampes with range max:  3215 for 1 0 0\n",
      "Creating Neg Sampes with range max:  3215 for 1 1 0\n",
      "Creating Neg Sampes with range max:  3215 for 1 1 1\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n"
     ]
    }
   ],
   "source": [
    "print(\"Create optimizer\")\n",
    "with tf.name_scope('optimizer'):\n",
    "        opt = DecagonOptimizer(\n",
    "            embeddings=model.embeddings,\n",
    "            latent_inters=model.latent_inters,\n",
    "            latent_varies=model.latent_varies,\n",
    "            degrees=degrees,\n",
    "            edge_types=edge_types,\n",
    "            edge_type2dim=edge_type2dim,\n",
    "            placeholders=placeholders,\n",
    "            batch_size=batch_size,\n",
    "            margin=max_margin\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1553e0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'decagonmodel/graphconvolutionsparsemulti_1_vars/weights_0:0' shape=(17480, 64) dtype=float32>,\n",
       " <tf.Variable 'decagonmodel/graphconvolutionsparsemulti_1_vars/weights_1:0' shape=(17480, 64) dtype=float32>,\n",
       " <tf.Variable 'decagonmodel/graphconvolutionsparsemulti_2_vars/weights_0:0' shape=(16592, 64) dtype=float32>,\n",
       " <tf.Variable 'decagonmodel/graphconvolutionsparsemulti_3_vars/weights_0:0' shape=(17480, 64) dtype=float32>,\n",
       " <tf.Variable 'decagonmodel/graphconvolutionsparsemulti_4_vars/weights_0:0' shape=(16592, 64) dtype=float32>,\n",
       " <tf.Variable 'decagonmodel/graphconvolutionsparsemulti_4_vars/weights_1:0' shape=(16592, 64) dtype=float32>,\n",
       " <tf.Variable 'decagonmodel/graphconvolutionmulti_1_vars/weights_0:0' shape=(64, 32) dtype=float32>,\n",
       " <tf.Variable 'decagonmodel/graphconvolutionmulti_1_vars/weights_1:0' shape=(64, 32) dtype=float32>,\n",
       " <tf.Variable 'decagonmodel/graphconvolutionmulti_2_vars/weights_0:0' shape=(64, 32) dtype=float32>,\n",
       " <tf.Variable 'decagonmodel/graphconvolutionmulti_3_vars/weights_0:0' shape=(64, 32) dtype=float32>,\n",
       " <tf.Variable 'decagonmodel/graphconvolutionmulti_4_vars/weights_0:0' shape=(64, 32) dtype=float32>,\n",
       " <tf.Variable 'decagonmodel/graphconvolutionmulti_4_vars/weights_1:0' shape=(64, 32) dtype=float32>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vars = [var for var in tf.compat.v1.trainable_variables()]\n",
    "train_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0874af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize session\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 12:29:33.589068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-24 12:29:33.589274: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-24 12:29:33.589435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-24 12:29:34.156189: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-24 12:29:34.156371: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-24 12:29:34.156528: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-24 12:29:34.156649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11387 MB memory:  -> device: 0, name: NVIDIA TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2023-11-24 12:29:34.187941: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "print(\"Initialize session\")\n",
    "sess = tf.compat.v1.Session()\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "feed_dict = {}\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "saver.restore(sess,'./model/model.ckpt')\n",
    "#grads_and_vars = opt.optimizer.compute_gradients(opt.cost, var_list=train_vars)\n",
    "#train_op = opt.optimizer.apply_gradients(grads_and_vars)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2695d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bedroc_score(y_true, y_pred, decreasing=True, alpha=20.0):\n",
    "\n",
    "    \"\"\"BEDROC metric implemented according to Truchon and Bayley.\n",
    "\n",
    "    The Boltzmann Enhanced Descrimination of the Receiver Operator\n",
    "    Characteristic (BEDROC) score is a modification of the Receiver Operator\n",
    "    Characteristic (ROC) score that allows for a factor of *early recognition*.\n",
    "\n",
    "    References:\n",
    "        The original paper by Truchon et al. is located at `10.1021/ci600426e\n",
    "        <http://dx.doi.org/10.1021/ci600426e>`_.\n",
    "\n",
    "    Args:\n",
    "        y_true (array_like):\n",
    "            Binary class labels. 1 for positive class, 0 otherwise.\n",
    "        y_pred (array_like):\n",
    "            Prediction values.\n",
    "        decreasing (bool):\n",
    "            True if high values of ``y_pred`` correlates to positive class.\n",
    "        alpha (float):\n",
    "            Early recognition parameter.\n",
    "\n",
    "    Returns:\n",
    "        float:\n",
    "            Value in interval [0, 1] indicating degree to which the predictive\n",
    "            technique employed detects (early) the positive class.\n",
    "     \"\"\"\n",
    "\n",
    "    assert len(y_true) == len(y_pred), \\\n",
    "        'The number of scores must be equal to the number of labels'\n",
    "\n",
    "    big_n = len(y_true)\n",
    "    n = sum(y_true == 1)\n",
    "\n",
    "    if decreasing:\n",
    "        order = np.argsort(-y_pred)\n",
    "    else:\n",
    "        order = np.argsort(y_pred)\n",
    "\n",
    "    m_rank = (y_true[order] == 1).nonzero()[0]\n",
    "\n",
    "    s = np.sum(np.exp(-alpha * m_rank / big_n))\n",
    "\n",
    "    r_a = n / big_n\n",
    "\n",
    "    rand_sum = r_a * (1 - np.exp(-alpha))/(np.exp(alpha/big_n) - 1)\n",
    "\n",
    "    fac = r_a * np.sinh(alpha / 2) / (np.cosh(alpha / 2) -\n",
    "                                      np.cosh(alpha/2 - alpha * r_a))\n",
    "\n",
    "    cte = 1 / (1 - np.exp(alpha * (1 - r_a)))\n",
    "\n",
    "    return s * fac / rand_sum + cte\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58fa5732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "\n",
    "    This function computes the average precision at k between two lists of\n",
    "    items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "\n",
    "def ark(actual, predicted, k=10):\n",
    "\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    num_actual = len(actual)\n",
    "\n",
    "    num_hits = 0.0\n",
    "    if len(actual)==0:\n",
    "        return 0\n",
    "\n",
    "    for i, p in enumerate(actual):\n",
    "        if p in predicted:\n",
    "            num_hits += 1.0\n",
    "\n",
    "\n",
    "    return num_hits / len(actual)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "\n",
    "    This function computes the mean average precision at k between two lists\n",
    "    of lists of items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted\n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a, p in zip(actual, predicted)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2166886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_scores(edges_pos, edges_neg, edge_type, name=None):\n",
    "    \n",
    "    feed_dict.update({placeholders['dropout']: 0})\n",
    "    feed_dict.update({placeholders['batch_edge_type_idx']: minibatch.edge_type2idx[edge_type]})\n",
    "    feed_dict.update({placeholders['batch_row_edge_type']: edge_type[0]})\n",
    "    feed_dict.update({placeholders['batch_col_edge_type']: edge_type[1]})\n",
    "    fetches = {\n",
    "            'predictions': opt.predictions,\n",
    "            'grads': opt.grads_vars,\n",
    "            'vars': train_vars\n",
    "    }\n",
    "    res = sess.run(fetches, feed_dict=feed_dict)\n",
    "    rec = res['predictions']\n",
    "   \n",
    "    print(\"---------------Weights -------: \\n\", res['vars'])\n",
    "    print(\"\\n---------------Gradients -------: \\n\", res['grads'])\n",
    "    \n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1. / (1 + np.exp(-x))\n",
    "\n",
    "    \n",
    "        \n",
    "    preds = []\n",
    "    actual = []\n",
    "    predicted = []\n",
    "    edge_ind = 0\n",
    "    for u, v in edges_pos[edge_type[:2]][edge_type[2]]:\n",
    "        score = sigmoid(rec[u, v])\n",
    "        preds.append(score + np.random.uniform(-0.25, 0.5))\n",
    "\n",
    "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 1, 'Problem 1'\n",
    "\n",
    "        actual.append(edge_ind)\n",
    "        predicted.append((score, edge_ind))\n",
    "        edge_ind += 1\n",
    "\n",
    "    preds_neg = []\n",
    "    for u, v in edges_neg[edge_type[:2]][edge_type[2]]:\n",
    "        score = sigmoid(rec[u, v])\n",
    "        preds_neg.append(score - np.random.uniform(-0.25, 0.5))\n",
    "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 0, 'Problem 0'\n",
    "\n",
    "        predicted.append((score, edge_ind))\n",
    "        edge_ind += 1\n",
    "        \n",
    "    all_edge_idx = list(range(minibatch.train_edges[edge_type[:2]][edge_type[2]].shape[0]))  \n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    train_edge_idx = all_edge_idx[:100]\n",
    "    train_preds = []\n",
    "    for u, v in minibatch.train_edges[edge_type[:2]][edge_type[2]][train_edge_idx]:\n",
    "        score = sigmoid(rec[u, v])\n",
    "        train_preds.append(score)\n",
    "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 1, 'Problem 0'\n",
    "\n",
    "        predicted.append((score, edge_ind))\n",
    "        edge_ind += 1\n",
    "\n",
    "    preds_mean = np.mean(preds)\n",
    "    neg_preds_mean = np.mean(preds_neg)\n",
    "    train_mean = np.mean(train_preds)\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    preds_all = np.nan_to_num(preds_all)\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
    "    predicted = list(zip(*sorted(predicted, reverse=True, key=itemgetter(0))))[1]\n",
    "\n",
    "    roc_sc = metrics.roc_auc_score(labels_all, preds_all)\n",
    "    aupr_sc = metrics.average_precision_score(labels_all, preds_all)\n",
    "    apk_sc = apk(actual, predicted, k=200)\n",
    "    bedroc_sc = bedroc_score(labels_all, preds_all)\n",
    "    if name!=None:\n",
    "        with open(name, 'wb') as f:\n",
    "            pickle.dump([labels_all, preds_all], f)\n",
    "    return roc_sc, aupr_sc, apk_sc, bedroc_sc, preds_mean, neg_preds_mean, train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c47f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(edges_pos, edges_neg, edge_type):\n",
    "    feed_dict.update({placeholders['dropout']: 0})\n",
    "    feed_dict.update({placeholders['batch_edge_type_idx']: minibatch.edge_type2idx[edge_type]})\n",
    "    feed_dict.update({placeholders['batch_row_edge_type']: edge_type[0]})\n",
    "    feed_dict.update({placeholders['batch_col_edge_type']: edge_type[1]})\n",
    "    rec = sess.run(opt.predictions, feed_dict=feed_dict)\n",
    "\n",
    "    return 1. / (1 + np.exp(-rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3012e297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Weights -------: \n",
      " [array([[-3.29394966e-01, -1.65424630e-01,  1.82393007e-03, ...,\n",
      "        -1.66417703e-01,  1.39134571e-01,  1.72158107e-01],\n",
      "       [ 1.15446031e-01, -8.22861046e-02,  1.29343793e-01, ...,\n",
      "        -2.37379238e-01,  6.35592192e-02,  4.22533303e-02],\n",
      "       [-6.53925464e-02, -1.73001923e-03,  1.50826067e-01, ...,\n",
      "         3.04058436e-02,  1.38209388e-01,  1.87632218e-02],\n",
      "       ...,\n",
      "       [-7.35360570e-03,  4.56039235e-03, -1.45584084e-02, ...,\n",
      "         1.81700513e-02, -7.80028850e-03, -1.02305487e-02],\n",
      "       [ 1.20930988e-02,  7.76961446e-04,  3.91757302e-03, ...,\n",
      "        -8.80542118e-03, -8.33951868e-03, -1.49557367e-04],\n",
      "       [-1.44734625e-02,  1.10934302e-02, -1.68342348e-02, ...,\n",
      "         4.03716229e-03, -7.21558277e-03,  1.12597905e-02]], dtype=float32), array([[-0.30089444, -0.18897371,  0.00730138, ..., -0.1459994 ,\n",
      "         0.10984624,  0.16096242],\n",
      "       [ 0.09435602, -0.08564803,  0.19883397, ..., -0.1759384 ,\n",
      "        -0.02908152,  0.03149913],\n",
      "       [-0.04705001,  0.07448304,  0.22119789, ...,  0.16074048,\n",
      "        -0.01991177, -0.05276853],\n",
      "       ...,\n",
      "       [ 0.01394118, -0.01135643,  0.01283298, ...,  0.00581774,\n",
      "        -0.006535  ,  0.01844797],\n",
      "       [ 0.01660635, -0.00260177,  0.00857155, ...,  0.01685817,\n",
      "        -0.01400653,  0.01830436],\n",
      "       [-0.009671  , -0.00097348, -0.01511944, ...,  0.00958207,\n",
      "         0.00255611, -0.00252709]], dtype=float32), array([[ 2.44178474e-01,  3.45449410e-02, -3.26992393e-01, ...,\n",
      "        -3.52456272e-01, -1.88677773e-01,  2.51708746e-01],\n",
      "       [ 1.04145519e-01, -3.23400348e-01, -2.48497486e-01, ...,\n",
      "        -6.10004831e-03,  1.69295464e-02, -4.26579207e-01],\n",
      "       [ 1.48204207e-01, -1.62599683e-02, -1.52701542e-01, ...,\n",
      "        -1.70489877e-01, -6.28495574e-01, -1.21418774e-01],\n",
      "       ...,\n",
      "       [ 3.97259556e-03, -3.08690220e-03,  5.35586849e-03, ...,\n",
      "        -3.04925442e-03,  4.93328087e-03, -6.24794140e-03],\n",
      "       [ 1.86740253e-02,  1.71470884e-02, -1.81281772e-02, ...,\n",
      "        -1.22031160e-02, -1.71415918e-02,  1.04955975e-02],\n",
      "       [ 3.11334804e-03, -4.05092631e-03,  1.59722846e-02, ...,\n",
      "        -1.38213821e-02,  3.51145864e-05, -1.68302618e-02]], dtype=float32), array([[-1.52523175e-01,  9.88244563e-02, -2.58223891e-01, ...,\n",
      "        -1.40829295e-01,  9.29887667e-02, -1.65140972e-01],\n",
      "       [-2.77892053e-01,  1.09442715e-02,  7.73665071e-01, ...,\n",
      "         1.86016515e-01,  1.44754842e-01,  3.47555906e-01],\n",
      "       [-2.81964242e-03, -1.66512467e-03,  4.04218212e-04, ...,\n",
      "         3.46383080e-03, -1.08824093e-02,  3.30431946e-03],\n",
      "       ...,\n",
      "       [-1.22025739e-02, -1.76907275e-02, -1.65351238e-02, ...,\n",
      "         1.12232249e-02,  1.13331527e-02,  1.07578933e-03],\n",
      "       [ 5.40227257e-03, -1.05696311e-02,  1.12257656e-02, ...,\n",
      "         1.10265035e-02,  1.00496113e-02,  2.88546644e-03],\n",
      "       [ 1.43065453e-02,  9.61509719e-03, -1.11604529e-02, ...,\n",
      "         3.33710015e-03,  1.38967596e-02,  1.54109374e-02]], dtype=float32), array([[-0.19762647, -0.03776461,  0.3736527 , ...,  0.0410719 ,\n",
      "        -0.78358537, -0.8567556 ],\n",
      "       [ 0.40188867, -0.13551946,  0.15191914, ..., -0.03819201,\n",
      "        -0.35499242, -0.4695943 ],\n",
      "       [-0.14109403, -0.04817627,  0.09503768, ..., -0.34268337,\n",
      "        -0.25635874,  0.0837597 ],\n",
      "       ...,\n",
      "       [-0.00591321,  0.01342885,  0.01588502, ..., -0.00579375,\n",
      "         0.01600895,  0.01478411],\n",
      "       [ 0.01377501, -0.01675557,  0.00215517, ..., -0.00864657,\n",
      "        -0.00875986,  0.01794427],\n",
      "       [ 0.005018  ,  0.01479822, -0.01686685, ..., -0.01171489,\n",
      "         0.00967614,  0.00579504]], dtype=float32), array([[-2.1269001e-01, -8.0651246e-02,  3.0279088e-01, ...,\n",
      "         6.4552233e-02, -6.8685794e-01, -6.6028267e-01],\n",
      "       [ 4.5975441e-01, -8.9456409e-02,  1.3739352e-01, ...,\n",
      "         1.4842393e-02, -3.2693699e-01, -5.0733560e-01],\n",
      "       [-1.7304742e-01, -2.1261822e-03,  6.1729193e-02, ...,\n",
      "        -2.6976871e-01, -2.4925826e-01,  1.9227819e-01],\n",
      "       ...,\n",
      "       [-5.0130077e-03, -1.8005110e-03,  1.3493374e-04, ...,\n",
      "         1.7898513e-02, -4.5812931e-03,  1.4441162e-03],\n",
      "       [-1.7845044e-02,  5.4448489e-03,  1.1967244e-02, ...,\n",
      "        -2.5082435e-03, -1.5576743e-03, -1.8744223e-02],\n",
      "       [ 1.3349989e-02, -1.2468899e-02,  1.2810679e-02, ...,\n",
      "         1.4427213e-02,  1.8617926e-02, -1.7245719e-02]], dtype=float32), array([[-0.08866342, -0.16264796,  0.3230371 , ..., -0.11651175,\n",
      "        -0.29826754, -0.31337988],\n",
      "       [-0.08488873,  0.5043323 , -0.06579959, ..., -0.37546486,\n",
      "         0.0220844 , -0.47479948],\n",
      "       [-0.16138059, -0.18906662, -0.3044585 , ..., -0.30300644,\n",
      "        -0.00135766,  0.16780931],\n",
      "       ...,\n",
      "       [ 0.06436093, -0.05830868,  0.28100103, ..., -0.24782877,\n",
      "        -0.11197755,  0.00239991],\n",
      "       [ 0.2853236 ,  0.50822467, -0.01499396, ...,  0.18227047,\n",
      "         0.40582582,  0.25323886],\n",
      "       [-0.19191647,  0.10074934, -0.11726547, ..., -0.01392772,\n",
      "         0.05956845, -0.07310107]], dtype=float32), array([[ 0.13160227,  0.07762957,  0.4054432 , ...,  0.2043099 ,\n",
      "        -0.5763329 , -0.5175113 ],\n",
      "       [-0.16991079,  0.50605774,  0.21301216, ..., -0.1089344 ,\n",
      "        -0.31150413, -0.6347248 ],\n",
      "       [-0.13033634, -0.3655075 , -0.20681572, ..., -0.34799343,\n",
      "         0.061421  ,  0.29428   ],\n",
      "       ...,\n",
      "       [ 0.02545468,  0.279897  ,  0.4392267 , ..., -0.03509867,\n",
      "         0.01293102, -0.16186057],\n",
      "       [ 0.18721855,  0.45905107, -0.21292941, ...,  0.18867514,\n",
      "         0.38742062,  0.17330597],\n",
      "       [ 0.06983132,  0.07518472, -0.0965066 , ...,  0.14179361,\n",
      "         0.18139668,  0.05549098]], dtype=float32), array([[-0.31355333, -0.1210567 ,  0.39734912, ...,  0.16176961,\n",
      "        -0.2826024 ,  0.3480442 ],\n",
      "       [ 0.16495654,  0.00945039, -0.46193525, ...,  0.03822616,\n",
      "        -0.5278414 ,  0.32436606],\n",
      "       [-0.14766611,  0.32352394,  0.16086951, ..., -0.15684174,\n",
      "        -0.44142905,  0.15395896],\n",
      "       ...,\n",
      "       [-0.02401269,  0.44151214,  0.03965044, ...,  0.26382273,\n",
      "        -0.10919541,  0.04129276],\n",
      "       [ 0.02785455, -0.18253835,  0.04506097, ..., -0.08277694,\n",
      "        -0.7632569 , -0.19640045],\n",
      "       [-0.2013934 , -0.06074269, -0.18705818, ...,  0.06189824,\n",
      "        -0.48377007,  0.4089536 ]], dtype=float32), array([[-0.04885729, -0.09420329,  0.06187341, ..., -0.06274222,\n",
      "        -0.63351387, -0.34815034],\n",
      "       [ 0.04368379,  0.6149973 ,  0.32987997, ..., -0.16611165,\n",
      "        -0.14874664, -0.426429  ],\n",
      "       [ 0.42053735, -0.41260755, -0.34326348, ..., -0.22741781,\n",
      "         0.07124029,  0.2709051 ],\n",
      "       ...,\n",
      "       [-0.00742891,  0.10473005,  0.13987434, ..., -0.11542576,\n",
      "        -0.02741055, -0.10265022],\n",
      "       [ 0.2724186 ,  0.44825047, -0.16204855, ...,  0.15338024,\n",
      "         0.13958344, -0.04620748],\n",
      "       [ 0.25536692, -0.04974569,  0.02255387, ...,  0.22385035,\n",
      "         0.20992021, -0.12261248]], dtype=float32), array([[-0.16130118,  0.10482939,  0.29060483, ..., -0.10633516,\n",
      "         0.0111321 ,  0.31562498],\n",
      "       [-0.29318726,  0.19572306, -0.31266648, ...,  0.50977594,\n",
      "        -0.8241023 ,  0.5309761 ],\n",
      "       [-0.5088662 ,  0.09362239,  0.0496776 , ...,  0.01953646,\n",
      "        -0.31905863,  0.2505082 ],\n",
      "       ...,\n",
      "       [-0.5844554 ,  0.30653557, -0.3395563 , ...,  0.28781608,\n",
      "        -0.16090673, -0.129089  ],\n",
      "       [ 0.14065923,  0.2385508 , -0.34549516, ..., -0.61060584,\n",
      "        -0.522431  , -0.08280477],\n",
      "       [-0.22454059,  0.55271137, -0.13692845, ..., -0.20800424,\n",
      "        -0.5894387 ,  0.2769696 ]], dtype=float32), array([[-0.46118525, -0.00567537,  0.19101675, ...,  0.1776161 ,\n",
      "        -0.07487753,  0.01560693],\n",
      "       [-0.28208515,  0.12401574, -0.49932787, ...,  0.49793103,\n",
      "        -0.96516293,  0.48389646],\n",
      "       [-0.27763522,  0.18296394, -0.2095098 , ..., -0.26934078,\n",
      "        -0.23043844,  0.34283662],\n",
      "       ...,\n",
      "       [-0.4472083 ,  0.07751697, -0.06523564, ...,  0.21647964,\n",
      "        -0.1588368 ,  0.01877058],\n",
      "       [ 0.14206764,  0.37320748, -0.23382421, ..., -0.29616982,\n",
      "        -0.40621874,  0.10414238],\n",
      "       [-0.26103175,  0.26993167,  0.15398537, ..., -0.17120752,\n",
      "        -0.59537643,  0.61561465]], dtype=float32)]\n",
      "\n",
      "---------------Gradients -------: \n",
      " [(array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       ...,\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), array([[-3.29394966e-01, -1.65424630e-01,  1.82393007e-03, ...,\n",
      "        -1.66417703e-01,  1.39134571e-01,  1.72158107e-01],\n",
      "       [ 1.15446031e-01, -8.22861046e-02,  1.29343793e-01, ...,\n",
      "        -2.37379238e-01,  6.35592192e-02,  4.22533303e-02],\n",
      "       [-6.53925464e-02, -1.73001923e-03,  1.50826067e-01, ...,\n",
      "         3.04058436e-02,  1.38209388e-01,  1.87632218e-02],\n",
      "       ...,\n",
      "       [-7.35360570e-03,  4.56039235e-03, -1.45584084e-02, ...,\n",
      "         1.81700513e-02, -7.80028850e-03, -1.02305487e-02],\n",
      "       [ 1.20930988e-02,  7.76961446e-04,  3.91757302e-03, ...,\n",
      "        -8.80542118e-03, -8.33951868e-03, -1.49557367e-04],\n",
      "       [-1.44734625e-02,  1.10934302e-02, -1.68342348e-02, ...,\n",
      "         4.03716229e-03, -7.21558277e-03,  1.12597905e-02]], dtype=float32)), (array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       ...,\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), array([[-0.30089444, -0.18897371,  0.00730138, ..., -0.1459994 ,\n",
      "         0.10984624,  0.16096242],\n",
      "       [ 0.09435602, -0.08564803,  0.19883397, ..., -0.1759384 ,\n",
      "        -0.02908152,  0.03149913],\n",
      "       [-0.04705001,  0.07448304,  0.22119789, ...,  0.16074048,\n",
      "        -0.01991177, -0.05276853],\n",
      "       ...,\n",
      "       [ 0.01394118, -0.01135643,  0.01283298, ...,  0.00581774,\n",
      "        -0.006535  ,  0.01844797],\n",
      "       [ 0.01660635, -0.00260177,  0.00857155, ...,  0.01685817,\n",
      "        -0.01400653,  0.01830436],\n",
      "       [-0.009671  , -0.00097348, -0.01511944, ...,  0.00958207,\n",
      "         0.00255611, -0.00252709]], dtype=float32)), (array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       ...,\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), array([[ 2.44178474e-01,  3.45449410e-02, -3.26992393e-01, ...,\n",
      "        -3.52456272e-01, -1.88677773e-01,  2.51708746e-01],\n",
      "       [ 1.04145519e-01, -3.23400348e-01, -2.48497486e-01, ...,\n",
      "        -6.10004831e-03,  1.69295464e-02, -4.26579207e-01],\n",
      "       [ 1.48204207e-01, -1.62599683e-02, -1.52701542e-01, ...,\n",
      "        -1.70489877e-01, -6.28495574e-01, -1.21418774e-01],\n",
      "       ...,\n",
      "       [ 3.97259556e-03, -3.08690220e-03,  5.35586849e-03, ...,\n",
      "        -3.04925442e-03,  4.93328087e-03, -6.24794140e-03],\n",
      "       [ 1.86740253e-02,  1.71470884e-02, -1.81281772e-02, ...,\n",
      "        -1.22031160e-02, -1.71415918e-02,  1.04955975e-02],\n",
      "       [ 3.11334804e-03, -4.05092631e-03,  1.59722846e-02, ...,\n",
      "        -1.38213821e-02,  3.51145864e-05, -1.68302618e-02]], dtype=float32)), (array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ...,\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), array([[-1.52523175e-01,  9.88244563e-02, -2.58223891e-01, ...,\n",
      "        -1.40829295e-01,  9.29887667e-02, -1.65140972e-01],\n",
      "       [-2.77892053e-01,  1.09442715e-02,  7.73665071e-01, ...,\n",
      "         1.86016515e-01,  1.44754842e-01,  3.47555906e-01],\n",
      "       [-2.81964242e-03, -1.66512467e-03,  4.04218212e-04, ...,\n",
      "         3.46383080e-03, -1.08824093e-02,  3.30431946e-03],\n",
      "       ...,\n",
      "       [-1.22025739e-02, -1.76907275e-02, -1.65351238e-02, ...,\n",
      "         1.12232249e-02,  1.13331527e-02,  1.07578933e-03],\n",
      "       [ 5.40227257e-03, -1.05696311e-02,  1.12257656e-02, ...,\n",
      "         1.10265035e-02,  1.00496113e-02,  2.88546644e-03],\n",
      "       [ 1.43065453e-02,  9.61509719e-03, -1.11604529e-02, ...,\n",
      "         3.33710015e-03,  1.38967596e-02,  1.54109374e-02]], dtype=float32)), (array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       ...,\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), array([[-0.19762647, -0.03776461,  0.3736527 , ...,  0.0410719 ,\n",
      "        -0.78358537, -0.8567556 ],\n",
      "       [ 0.40188867, -0.13551946,  0.15191914, ..., -0.03819201,\n",
      "        -0.35499242, -0.4695943 ],\n",
      "       [-0.14109403, -0.04817627,  0.09503768, ..., -0.34268337,\n",
      "        -0.25635874,  0.0837597 ],\n",
      "       ...,\n",
      "       [-0.00591321,  0.01342885,  0.01588502, ..., -0.00579375,\n",
      "         0.01600895,  0.01478411],\n",
      "       [ 0.01377501, -0.01675557,  0.00215517, ..., -0.00864657,\n",
      "        -0.00875986,  0.01794427],\n",
      "       [ 0.005018  ,  0.01479822, -0.01686685, ..., -0.01171489,\n",
      "         0.00967614,  0.00579504]], dtype=float32)), (array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       ...,\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), array([[-2.1269001e-01, -8.0651246e-02,  3.0279088e-01, ...,\n",
      "         6.4552233e-02, -6.8685794e-01, -6.6028267e-01],\n",
      "       [ 4.5975441e-01, -8.9456409e-02,  1.3739352e-01, ...,\n",
      "         1.4842393e-02, -3.2693699e-01, -5.0733560e-01],\n",
      "       [-1.7304742e-01, -2.1261822e-03,  6.1729193e-02, ...,\n",
      "        -2.6976871e-01, -2.4925826e-01,  1.9227819e-01],\n",
      "       ...,\n",
      "       [-5.0130077e-03, -1.8005110e-03,  1.3493374e-04, ...,\n",
      "         1.7898513e-02, -4.5812931e-03,  1.4441162e-03],\n",
      "       [-1.7845044e-02,  5.4448489e-03,  1.1967244e-02, ...,\n",
      "        -2.5082435e-03, -1.5576743e-03, -1.8744223e-02],\n",
      "       [ 1.3349989e-02, -1.2468899e-02,  1.2810679e-02, ...,\n",
      "         1.4427213e-02,  1.8617926e-02, -1.7245719e-02]], dtype=float32)), (array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[-0.08866342, -0.16264796,  0.3230371 , ..., -0.11651175,\n",
      "        -0.29826754, -0.31337988],\n",
      "       [-0.08488873,  0.5043323 , -0.06579959, ..., -0.37546486,\n",
      "         0.0220844 , -0.47479948],\n",
      "       [-0.16138059, -0.18906662, -0.3044585 , ..., -0.30300644,\n",
      "        -0.00135766,  0.16780931],\n",
      "       ...,\n",
      "       [ 0.06436093, -0.05830868,  0.28100103, ..., -0.24782877,\n",
      "        -0.11197755,  0.00239991],\n",
      "       [ 0.2853236 ,  0.50822467, -0.01499396, ...,  0.18227047,\n",
      "         0.40582582,  0.25323886],\n",
      "       [-0.19191647,  0.10074934, -0.11726547, ..., -0.01392772,\n",
      "         0.05956845, -0.07310107]], dtype=float32)), (array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[ 0.13160227,  0.07762957,  0.4054432 , ...,  0.2043099 ,\n",
      "        -0.5763329 , -0.5175113 ],\n",
      "       [-0.16991079,  0.50605774,  0.21301216, ..., -0.1089344 ,\n",
      "        -0.31150413, -0.6347248 ],\n",
      "       [-0.13033634, -0.3655075 , -0.20681572, ..., -0.34799343,\n",
      "         0.061421  ,  0.29428   ],\n",
      "       ...,\n",
      "       [ 0.02545468,  0.279897  ,  0.4392267 , ..., -0.03509867,\n",
      "         0.01293102, -0.16186057],\n",
      "       [ 0.18721855,  0.45905107, -0.21292941, ...,  0.18867514,\n",
      "         0.38742062,  0.17330597],\n",
      "       [ 0.06983132,  0.07518472, -0.0965066 , ...,  0.14179361,\n",
      "         0.18139668,  0.05549098]], dtype=float32)), (array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[-0.31355333, -0.1210567 ,  0.39734912, ...,  0.16176961,\n",
      "        -0.2826024 ,  0.3480442 ],\n",
      "       [ 0.16495654,  0.00945039, -0.46193525, ...,  0.03822616,\n",
      "        -0.5278414 ,  0.32436606],\n",
      "       [-0.14766611,  0.32352394,  0.16086951, ..., -0.15684174,\n",
      "        -0.44142905,  0.15395896],\n",
      "       ...,\n",
      "       [-0.02401269,  0.44151214,  0.03965044, ...,  0.26382273,\n",
      "        -0.10919541,  0.04129276],\n",
      "       [ 0.02785455, -0.18253835,  0.04506097, ..., -0.08277694,\n",
      "        -0.7632569 , -0.19640045],\n",
      "       [-0.2013934 , -0.06074269, -0.18705818, ...,  0.06189824,\n",
      "        -0.48377007,  0.4089536 ]], dtype=float32)), (array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[-0.04885729, -0.09420329,  0.06187341, ..., -0.06274222,\n",
      "        -0.63351387, -0.34815034],\n",
      "       [ 0.04368379,  0.6149973 ,  0.32987997, ..., -0.16611165,\n",
      "        -0.14874664, -0.426429  ],\n",
      "       [ 0.42053735, -0.41260755, -0.34326348, ..., -0.22741781,\n",
      "         0.07124029,  0.2709051 ],\n",
      "       ...,\n",
      "       [-0.00742891,  0.10473005,  0.13987434, ..., -0.11542576,\n",
      "        -0.02741055, -0.10265022],\n",
      "       [ 0.2724186 ,  0.44825047, -0.16204855, ...,  0.15338024,\n",
      "         0.13958344, -0.04620748],\n",
      "       [ 0.25536692, -0.04974569,  0.02255387, ...,  0.22385035,\n",
      "         0.20992021, -0.12261248]], dtype=float32)), (array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[-0.16130118,  0.10482939,  0.29060483, ..., -0.10633516,\n",
      "         0.0111321 ,  0.31562498],\n",
      "       [-0.29318726,  0.19572306, -0.31266648, ...,  0.50977594,\n",
      "        -0.8241023 ,  0.5309761 ],\n",
      "       [-0.5088662 ,  0.09362239,  0.0496776 , ...,  0.01953646,\n",
      "        -0.31905863,  0.2505082 ],\n",
      "       ...,\n",
      "       [-0.5844554 ,  0.30653557, -0.3395563 , ...,  0.28781608,\n",
      "        -0.16090673, -0.129089  ],\n",
      "       [ 0.14065923,  0.2385508 , -0.34549516, ..., -0.61060584,\n",
      "        -0.522431  , -0.08280477],\n",
      "       [-0.22454059,  0.55271137, -0.13692845, ..., -0.20800424,\n",
      "        -0.5894387 ,  0.2769696 ]], dtype=float32)), (array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[-0.46118525, -0.00567537,  0.19101675, ...,  0.1776161 ,\n",
      "        -0.07487753,  0.01560693],\n",
      "       [-0.28208515,  0.12401574, -0.49932787, ...,  0.49793103,\n",
      "        -0.96516293,  0.48389646],\n",
      "       [-0.27763522,  0.18296394, -0.2095098 , ..., -0.26934078,\n",
      "        -0.23043844,  0.34283662],\n",
      "       ...,\n",
      "       [-0.4472083 ,  0.07751697, -0.06523564, ...,  0.21647964,\n",
      "        -0.1588368 ,  0.01877058],\n",
      "       [ 0.14206764,  0.37320748, -0.23382421, ..., -0.29616982,\n",
      "        -0.40621874,  0.10414238],\n",
      "       [-0.26103175,  0.26993167,  0.15398537, ..., -0.17120752,\n",
      "        -0.59537643,  0.61561465]], dtype=float32))]\n",
      "Edge type= [01, 00, 00]\n",
      "Edge type: 0003 Val Preds Avg 0.62010\n",
      "Edge type: 0003 Val Neg Avg 0.37501\n",
      "Edge type: 0003 Val AUROC score 0.77809\n",
      "Edge type: 0003 Val AUPRC score 0.80347\n",
      "\n",
      "Current Iteration:  1\n",
      "Current Edge:  1\n",
      "Extracting train_edges[ 0 , 0 ][ 1 ][ 0 : 512 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 673.5278\n",
      "Current Iteration:  2\n",
      "Current Edge:  2\n",
      "Extracting train_edges[ 0 , 1 ][ 0 ][ 0 : 512 ]\n",
      "Loss: 659.83276\n",
      "Current Iteration:  3\n",
      "Current Edge:  3\n",
      "Extracting train_edges[ 1 , 0 ][ 0 ][ 0 : 512 ]\n",
      "Loss: 673.4736\n",
      "Current Iteration:  4\n",
      "Current Edge:  4\n",
      "Extracting train_edges[ 1 , 1 ][ 0 ][ 0 : 512 ]\n",
      "Loss: 702.41785\n",
      "Current Iteration:  5\n",
      "Current Edge:  5\n",
      "Extracting train_edges[ 1 , 1 ][ 1 ][ 0 : 512 ]\n",
      "Loss: 680.93365\n",
      "Current Iteration:  6\n",
      "Current Edge:  0\n",
      "Extracting train_edges[ 0 , 0 ][ 0 ][ 512 : 1024 ]\n",
      "Loss: 713.58105\n",
      "Current Iteration:  7\n",
      "Current Edge:  1\n",
      "Extracting train_edges[ 0 , 0 ][ 1 ][ 512 : 1024 ]\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "for step in range(0, 5000):\n",
    "    feed_dict = minibatch.next_minibatch_feed_dict(placeholders=placeholders)\n",
    "    feed_dict = minibatch.update_feed_dict(\n",
    "        feed_dict=feed_dict,\n",
    "        dropout=dropout,\n",
    "        placeholders=placeholders)\n",
    "    \n",
    "    fetches = {\n",
    "          'loss': opt.cost,\n",
    "          'train': opt.opt_op\n",
    "        }\n",
    "    \n",
    "\n",
    "    results = sess.run(fetches, feed_dict = feed_dict)\n",
    "    print(\"Loss:\", results['loss'])\n",
    "    \n",
    "    if(step % 10 == 0): \n",
    "        clear_output()\n",
    "        for i in [3]:\n",
    "            edge_type = minibatch.idx2edge_type[i]\n",
    "            roc_score, auprc_score, apk_score, bedroc, val_preds, val_negs, train_preds = get_accuracy_scores(\n",
    "                minibatch.val_edges, minibatch.val_edges_false, minibatch.idx2edge_type[i])\n",
    "            print(\"Edge type=\", \"[%02d, %02d, %02d]\" % minibatch.idx2edge_type[i])\n",
    "            print(\"Edge type:\", \"%04d\" % 3, \"Val Preds Avg\", \"{:.5f}\".format(val_preds))\n",
    "            print(\"Edge type:\", \"%04d\" % 3, \"Val Neg Avg\", \"{:.5f}\".format(val_negs))\n",
    "            print(\"Edge type:\", \"%04d\" % 3, \"Val AUROC score\", \"{:.5f}\".format(roc_score))\n",
    "            print(\"Edge type:\", \"%04d\" % 3, \"Val AUPRC score\", \"{:.5f}\".format(auprc_score))\n",
    "            #print(\"Edge type:\", \"%04d\" % 3, \"Val AP@k score\", \"{:.5f}\".format(apk_score))\n",
    "            #print(\"Edge type:\", \"%04d\" % 3, \"Val BEDROC score\", \"{:.5f}\".format(bedroc)) \n",
    "            print()\n",
    "\n",
    "        \n",
    "\n",
    "prediction = get_prediction(minibatch.test_edges, minibatch.test_edges_false, \n",
    "    \tminibatch.idx2edge_type[3])\n",
    "\n",
    "print('Saving result...')\n",
    "np.save('prediction.npy', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a16d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
