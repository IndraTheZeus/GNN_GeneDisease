{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ca77e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     active environment : GNNGeneDisease\n",
      "    active env location : /home/indradutta/miniconda3/envs/GNNGeneDisease\n",
      "            shell level : 2\n",
      "       user config file : /home/indradutta/.condarc\n",
      " populated config files : \n",
      "          conda version : 23.9.0\n",
      "    conda-build version : not installed\n",
      "         python version : 3.11.4.final.0\n",
      "       virtual packages : __archspec=1=x86_64\n",
      "                          __cuda=12.0=0\n",
      "                          __glibc=2.35=0\n",
      "                          __linux=6.2.0=0\n",
      "                          __unix=0=0\n",
      "       base environment : /home/indradutta/miniconda3  (writable)\n",
      "      conda av data dir : /home/indradutta/miniconda3/etc/conda\n",
      "  conda av metadata url : None\n",
      "           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64\n",
      "                          https://repo.anaconda.com/pkgs/main/noarch\n",
      "                          https://repo.anaconda.com/pkgs/r/linux-64\n",
      "                          https://repo.anaconda.com/pkgs/r/noarch\n",
      "          package cache : /home/indradutta/miniconda3/pkgs\n",
      "                          /home/indradutta/.conda/pkgs\n",
      "       envs directories : /home/indradutta/miniconda3/envs\n",
      "                          /home/indradutta/.conda/envs\n",
      "               platform : linux-64\n",
      "             user-agent : conda/23.9.0 requests/2.31.0 CPython/3.11.4 Linux/6.2.0-36-generic ubuntu/22.04.2 glibc/2.35\n",
      "                UID:GID : 1119:1128\n",
      "             netrc file : None\n",
      "           offline mode : False\n",
      "\n",
      "\n",
      "data_prioritization\t\t model\t     README.md\n",
      "Disease_gene_prioritization_GCN  PGCN.ipynb\n"
     ]
    }
   ],
   "source": [
    "!conda info #Working Environment is GNNGeneDisease\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "890bb3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 12:51:33.760270: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-22 12:51:33.760314: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-22 12:51:33.760353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from operator import itemgetter\n",
    "from itertools import combinations\n",
    "import time\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as sio\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ce2004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 12:51:36.566531: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-22 12:51:36.572135: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-22 12:51:36.572337: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "924ddd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5e92f",
   "metadata": {},
   "source": [
    "# DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6043fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD Disease Gene Adjacency Network\n",
    "gene_phenes_path = './data_prioritization/genes_phenes.mat'\n",
    "f = h5py.File(gene_phenes_path, 'r')\n",
    "gene_network_adj = sp.csc_matrix((np.array(f['GeneGene_Hs']['data']),\n",
    "    np.array(f['GeneGene_Hs']['ir']), np.array(f['GeneGene_Hs']['jc'])),\n",
    "    shape=(12331,12331))\n",
    "gene_network_adj = gene_network_adj.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae15f0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "875c7bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_edge_threshold(network_adj, threshold):\n",
    "    edge_tmp, edge_value, shape_tmp = sparse_to_tuple(network_adj)\n",
    "    preserved_edge_index = np.where(edge_value>threshold)[0]\n",
    "    preserved_network = sp.csr_matrix(\n",
    "        (edge_value[preserved_edge_index], \n",
    "        (edge_tmp[preserved_edge_index,0], edge_tmp[preserved_edge_index, 1])),\n",
    "        shape=shape_tmp)\n",
    "    return preserved_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc999264",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_network_adj = sp.csc_matrix((np.array(f['PhenotypeSimilarities']['data']),\n",
    "    np.array(f['PhenotypeSimilarities']['ir']), np.array(f['PhenotypeSimilarities']['jc'])),\n",
    "    shape=(3215, 3215))\n",
    "disease_network_adj = disease_network_adj.tocsr()  ## CONVERTED TO SPARSE ROW\n",
    "# >0.2 values get preserved\n",
    "disease_network_adj = network_edge_threshold(disease_network_adj, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7dc1911",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.200001 0.200002 0.200003 ... 0.999965 0.999998 1.      ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<3215x3215 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 645965 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.unique(disease_network_adj.tocoo().data))\n",
    "disease_network_adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75a032a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: Gene Disease Adj is 1, 0 matrix, while gene_gene (12331, 12331)\n",
    "##        and disease-disease ( 3215, 3215) have edge level values\n",
    "\n",
    "dg_ref = f['GenePhene'][0][0]\n",
    "gene_disease_adj = sp.csc_matrix((np.array(f[dg_ref]['data']),\n",
    "    np.array(f[dg_ref]['ir']), np.array(f[dg_ref]['jc'])),\n",
    "    shape=(12331, 3215))\n",
    "gene_disease_adj = gene_disease_adj.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94e7d7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"data\": shape (3954,), type \"<f8\">"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[dg_ref]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53e08c0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12331, 3215)\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "print(gene_disease_adj.shape)\n",
    "print(np.unique(gene_disease_adj.tocoo().data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1fe014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WHAT DATA is THIS ? 34 novel associtaions of value 1, 0\n",
    "novel_associations_adj = sp.csc_matrix((np.array(f['NovelAssociations']['data']),\n",
    "    np.array(f['NovelAssociations']['ir']), np.array(f['NovelAssociations']['jc'])),\n",
    "    shape=(12331,3215))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ab85a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"data\": shape (34,), type \"<f8\">"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['NovelAssociations']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7977cb8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12331, 3215)\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "print(novel_associations_adj.shape)\n",
    "print(np.unique(novel_associations_adj.tocoo().data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9faec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature value of each Gene: 4536 features per gene\n",
    "gene_feature_path = './Disease_gene_prioritization_GCN/data_prioritization/GeneFeatures.mat'\n",
    "f_gene_feature = h5py.File(gene_feature_path,'r')\n",
    "gene_feature_exp = np.array(f_gene_feature['GeneFeatures'])\n",
    "gene_feature_exp = np.transpose(gene_feature_exp)\n",
    "gene_network_exp = sp.csc_matrix(gene_feature_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "587eab04",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12331, 4536)\n",
      "[-2.5184551  -2.46199675 -2.46052243 ...  6.37752569  6.56798025\n",
      "  6.62628665]\n"
     ]
    }
   ],
   "source": [
    "print(gene_network_exp.shape)\n",
    "print(np.unique(gene_network_exp.tocoo().data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de76070f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"GenePhene\": shape (9, 1), type \"|O\">"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(f[dg_ref]['data'])\n",
    "f['GenePhene']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d2cfb7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12331, 1137)\n",
      "[2.28832952e-03 2.50000000e-02 3.57142857e-02 4.00000000e-02\n",
      " 4.34782609e-02 5.00000000e-02 5.26315789e-02 6.25000000e-02\n",
      " 6.66666667e-02 7.14285714e-02 7.69230769e-02 8.33333333e-02\n",
      " 8.69565217e-02 9.09090909e-02 1.00000000e-01 1.11111111e-01\n",
      " 1.17647059e-01 1.25000000e-01 1.30434783e-01 1.42857143e-01\n",
      " 1.53846154e-01 1.66666667e-01 1.73913043e-01 1.81818182e-01\n",
      " 1.87500000e-01 2.00000000e-01 2.14285714e-01 2.17391304e-01\n",
      " 2.22222222e-01 2.30769231e-01 2.50000000e-01 2.85714286e-01\n",
      " 3.00000000e-01 3.33333333e-01 3.63636364e-01 3.75000000e-01\n",
      " 3.84615385e-01 4.00000000e-01 4.44444444e-01 5.00000000e-01\n",
      " 5.71428571e-01 6.00000000e-01 6.66666667e-01 7.50000000e-01\n",
      " 8.00000000e-01 8.33333333e-01 1.00000000e+00 1.16666667e+00\n",
      " 1.28571429e+00 1.33333333e+00 1.50000000e+00 1.66666667e+00\n",
      " 1.72727273e+00 1.75000000e+00 2.00000000e+00 2.16666667e+00\n",
      " 2.25000000e+00 2.33333333e+00 2.50000000e+00 2.66666667e+00\n",
      " 3.00000000e+00 3.50000000e+00 4.00000000e+00 4.33333333e+00\n",
      " 5.00000000e+00 6.00000000e+00 7.00000000e+00 8.00000000e+00\n",
      " 9.00000000e+00 1.00000000e+01 1.10000000e+01 1.20000000e+01\n",
      " 1.30000000e+01 1.40000000e+01 1.90000000e+01 2.00000000e+01]\n",
      "(12331, 744)\n",
      "[0.02941176 0.04761905 0.05       0.05263158 0.05555556 0.0625\n",
      " 0.07142857 0.09090909 0.1        0.11111111 0.125      0.14285714\n",
      " 0.15384615 0.16666667 0.18181818 0.2        0.21052632 0.21428571\n",
      " 0.23076923 0.25       0.26315789 0.27777778 0.28571429 0.31578947\n",
      " 0.33333333 0.35714286 0.4        0.42857143 0.47368421 0.5\n",
      " 0.52631579 0.55555556 0.57142857 0.6        0.64285714 0.66666667\n",
      " 0.71428571 0.72222222 0.75       0.78571429 0.84210526 0.88888889\n",
      " 0.89473684 1.         1.07142857 1.14285714 1.33333333 1.5\n",
      " 2.         3.         4.        ]\n",
      "(12331, 2503)\n",
      "[0.00746269 0.05263158 0.08333333 0.09090909 0.1        0.11111111\n",
      " 0.125      0.14285714 0.15       0.16666667 0.2        0.25\n",
      " 0.28571429 0.33333333 0.4        0.5        0.66666667 1.\n",
      " 1.5        2.         3.        ]\n",
      "(12331, 1143)\n",
      "[0.08333333 0.13333333 0.14285714 0.2        0.25       0.33333333\n",
      " 0.5        0.66666667 1.         2.         3.        ]\n",
      "(12331, 324)\n",
      "[0.03571429 0.0625     0.09090909 0.1        0.11111111 0.125\n",
      " 0.14285714 0.16666667 0.1875     0.2        0.25       0.28571429\n",
      " 0.3125     0.33333333 0.375      0.4        0.5        0.6\n",
      " 0.66666667 0.8        1.         1.2        1.33333333 1.5\n",
      " 1.66666667 2.         3.         4.        ]\n",
      "(12331, 1188)\n",
      "[0.33333333 0.5        1.         2.        ]\n",
      "(12331, 4662)\n",
      "[0.09090909 0.125      0.14285714 0.2        0.22222222 0.25\n",
      " 0.33333333 0.5        0.66666667 1.         2.         3.        ]\n",
      "(12331, 1243)\n",
      "[2.12314225e-03 4.54545455e-02 5.00000000e-02 5.26315789e-02\n",
      " 5.55555556e-02 5.88235294e-02 6.66666667e-02 7.14285714e-02\n",
      " 7.69230769e-02 8.33333333e-02 9.09090909e-02 1.00000000e-01\n",
      " 1.11111111e-01 1.25000000e-01 1.42857143e-01 1.66666667e-01\n",
      " 1.81818182e-01 2.00000000e-01 2.50000000e-01 2.85714286e-01\n",
      " 3.33333333e-01 4.00000000e-01 4.28571429e-01 5.00000000e-01\n",
      " 5.71428571e-01 6.66666667e-01 1.00000000e+00 1.50000000e+00\n",
      " 2.00000000e+00 2.50000000e+00 3.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# // TODO: Explore why 1 - 9 and not 0 - 9\n",
    "## NOTE THIS DATA is different from gene_disease_adj  \n",
    "\n",
    "row_list = [3215, 1137, 744, 2503, 1143, 324, 1188, 4662, 1243]\n",
    "gene_feature_list_other_spe = list()\n",
    "for i in range(1,9):\n",
    "    dg_ref = f['GenePhene'][i][0]\n",
    "    disease_gene_adj_tmp = sp.csc_matrix((np.array(f[dg_ref]['data']),\n",
    "        np.array(f[dg_ref]['ir']), np.array(f[dg_ref]['jc'])),\n",
    "        shape=(12331, row_list[i]))\n",
    "    print(disease_gene_adj_tmp.shape)\n",
    "    print(np.unique(disease_gene_adj_tmp.tocoo().data))\n",
    "    gene_feature_list_other_spe.append(disease_gene_adj_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7ec1458",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<12331x1137 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 12010 stored elements in Compressed Sparse Column format>,\n",
       " <12331x744 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 30519 stored elements in Compressed Sparse Column format>,\n",
       " <12331x2503 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 68525 stored elements in Compressed Sparse Column format>,\n",
       " <12331x1143 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 4500 stored elements in Compressed Sparse Column format>,\n",
       " <12331x324 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 72846 stored elements in Compressed Sparse Column format>,\n",
       " <12331x1188 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 22150 stored elements in Compressed Sparse Column format>,\n",
       " <12331x4662 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 75199 stored elements in Compressed Sparse Column format>,\n",
       " <12331x1243 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 73284 stored elements in Compressed Sparse Column format>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " gene_feature_list_other_spe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26646523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3215, 16592)\n",
      "[2.05421771e-04 2.72671021e-04 2.87821641e-04 ... 9.20864777e-01\n",
      " 9.26488623e-01 9.86390140e-01]\n"
     ]
    }
   ],
   "source": [
    "disease_tfidf_path = './Disease_gene_prioritization_GCN/data_prioritization/clinicalfeatures_tfidf.mat'\n",
    "f_disease_tfidf = h5py.File(disease_tfidf_path)\n",
    "disease_tfidf = np.array(f_disease_tfidf['F'])\n",
    "disease_tfidf = np.transpose(disease_tfidf)\n",
    "disease_tfidf = sp.csc_matrix(disease_tfidf)\n",
    "print(disease_tfidf.shape)\n",
    "print(np.unique(disease_tfidf.tocoo().data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67482211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<3215x3215 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 645965 stored elements in Compressed Sparse Row format>]\n"
     ]
    }
   ],
   "source": [
    "dis_dis_adj_list= list()\n",
    "dis_dis_adj_list.append(disease_network_adj)\n",
    "print(dis_dis_adj_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6657a5",
   "metadata": {},
   "source": [
    "# DATA PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b13a917f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[[68.92149565  9.07395628 13.86730759 ...  0.          0.\n",
      "   0.        ]]\n",
      "(1, 12331)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12331,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_test_size = 0.1\n",
    "n_genes = 12331\n",
    "n_dis = 3215\n",
    "n_dis_rel_types = len(dis_dis_adj_list)\n",
    "print(n_dis_rel_types)\n",
    "gene_adj = gene_network_adj\n",
    "gene_degrees = np.array(gene_adj.sum(axis=0)).squeeze()\n",
    "print(gene_adj.sum(axis=0))\n",
    "print(gene_adj.sum(axis=0).shape)\n",
    "gene_degrees.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c75d1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12331, 3215)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3215,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_dis_adj = gene_disease_adj\n",
    "print(gene_dis_adj.shape)\n",
    "dis_gene_adj = gene_dis_adj.transpose(copy=True)\n",
    "dis_degrees_list = [np.array(dis_adj.sum(axis=0)).squeeze() for dis_adj in dis_dis_adj_list]\n",
    "dis_degrees_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96af076f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [array([68.92149565,  9.07395628, 13.86730759, ...,  0.        ,\n",
      "        0.        ,  0.        ]), array([68.92149565,  9.07395628, 13.86730759, ...,  0.        ,\n",
      "        0.        ,  0.        ])], 1: [array([ 1.      ,  1.      , 91.266734, ...,  7.260462,  1.      ,\n",
      "        1.      ]), array([ 1.      ,  1.      , 91.266734, ...,  7.260462,  1.      ,\n",
      "        1.      ])]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(0,\n",
       "  0): [<12331x12331 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 733836 stored elements in Compressed Sparse Row format>, <12331x12331 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 733836 stored elements in Compressed Sparse Column format>],\n",
       " (0,\n",
       "  1): [<12331x3215 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 3954 stored elements in Compressed Sparse Row format>],\n",
       " (1,\n",
       "  0): [<3215x12331 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 3954 stored elements in Compressed Sparse Column format>],\n",
       " (1,\n",
       "  1): [<3215x3215 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 645965 stored elements in Compressed Sparse Row format>,\n",
       "  <3215x3215 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 645965 stored elements in Compressed Sparse Column format>]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_mats_orig = {\n",
    "    (0, 0): [gene_adj, gene_adj.transpose(copy=True)],\n",
    "    (0, 1): [gene_dis_adj],\n",
    "    (1, 0): [dis_gene_adj],\n",
    "    (1, 1): dis_dis_adj_list + [x.transpose(copy=True) for x in dis_dis_adj_list],\n",
    "}\n",
    "degrees = {\n",
    "    0: [gene_degrees, gene_degrees],\n",
    "    1: dis_degrees_list + dis_degrees_list,\n",
    "}\n",
    "print(degrees)\n",
    "adj_mats_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "000198df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12331, 17480)\n",
      "(3215, 16592)\n"
     ]
    }
   ],
   "source": [
    "# Gene Feature Exp - Feature vector  + other 8 species\n",
    "gene_feat = sp.hstack(gene_feature_list_other_spe+[gene_feature_exp])\n",
    "print(gene_feat.shape)\n",
    "gene_nonzero_feat, gene_num_feat = gene_feat.shape\n",
    "gene_feat = sparse_to_tuple(gene_feat.tocoo())\n",
    "\n",
    "dis_feat = disease_tfidf\n",
    "dis_nonzero_feat, dis_num_feat = dis_feat.shape\n",
    "print(dis_feat.shape)\n",
    "dis_feat = sparse_to_tuple(dis_feat.tocoo())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5265eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 17480, 1: 16592}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 12331, 1: 3215}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_feat = {\n",
    "    0: gene_num_feat,\n",
    "    1: dis_num_feat,\n",
    "}\n",
    "nonzero_feat = {\n",
    "    0: gene_nonzero_feat,\n",
    "    1: dis_nonzero_feat,\n",
    "}\n",
    "feat = {\n",
    "    0: gene_feat,\n",
    "    1: dis_feat,\n",
    "}\n",
    "print(num_feat)\n",
    "nonzero_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "933d2589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge types: 6\n",
      "{(0, 0): 2, (0, 1): 1, (1, 0): 1, (1, 1): 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(0, 0): [(12331, 12331), (12331, 12331)],\n",
       " (0, 1): [(12331, 3215)],\n",
       " (1, 0): [(3215, 12331)],\n",
       " (1, 1): [(3215, 3215), (3215, 3215)]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_type2dim = {k: [adj.shape for adj in adjs] for k, adjs in adj_mats_orig.items()}\n",
    "# edge_type2decoder = {\n",
    "#     (0, 0): 'bilinear',\n",
    "#     (0, 1): 'bilinear',\n",
    "#     (1, 0): 'bilinear',\n",
    "#     (1, 1): 'bilinear',\n",
    "# }\n",
    "\n",
    "edge_type2decoder = {\n",
    "    (0, 0): 'innerproduct',\n",
    "    (0, 1): 'innerproduct',\n",
    "    (1, 0): 'innerproduct',\n",
    "    (1, 1): 'innerproduct',\n",
    "}\n",
    "\n",
    "edge_types = {k: len(v) for k, v in adj_mats_orig.items()}\n",
    "num_edge_types = sum(edge_types.values())\n",
    "print(\"Edge types:\", \"%d\" % num_edge_types)\n",
    "\n",
    "print(edge_types)\n",
    "edge_type2dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbc5c07",
   "metadata": {},
   "source": [
    "# INSIDE MAIN CALLING CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72b24282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "del_all_flags(tf.compat.v1.app.flags.FLAGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603abbc6",
   "metadata": {},
   "source": [
    "# FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fc5a7ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_sample_size =  1  #Negative sample size\n",
    "learning_rate =  0.001 # 'Initial learning rate\n",
    "hidden1_units =  64 #'Number of units in hidden layer 1\n",
    "hidden2_units = 32 #Number of units in hidden layer 2\n",
    "weight_decay =  0.001  #Weight for L2 loss on embedding matrix\n",
    "dropout = 0.1 # Dropout rate (1 - keep probability)\n",
    "max_margin =  0.1 #Max margin parameter in hinge loss\n",
    "batch_size = 512\n",
    "bias = True #Bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc39102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_placeholders(edge_types):\n",
    "    placeholders = {\n",
    "        'batch': tf.compat.v1.placeholder(tf.int32, name='batch'),\n",
    "        'batch_edge_type_idx': tf.compat.v1.placeholder(tf.int32, shape=(), name='batch_edge_type_idx'),\n",
    "        'batch_row_edge_type': tf.compat.v1.placeholder(tf.int32, shape=(), name='batch_row_edge_type'),\n",
    "        'batch_col_edge_type': tf.compat.v1.placeholder(tf.int32, shape=(), name='batch_col_edge_type'),\n",
    "        'degrees': tf.compat.v1.placeholder(tf.int32),\n",
    "        'dropout': tf.compat.v1.placeholder_with_default(0., shape=()),\n",
    "    }\n",
    "    placeholders.update({\n",
    "        'adj_mats_%d,%d,%d' % (i, j, k): tf.compat.v1.sparse_placeholder(tf.float32)\n",
    "        for i, j in edge_types for k in range(edge_types[i,j])})\n",
    "    placeholders.update({\n",
    "        'feat_%d' % i: tf.compat.v1.sparse_placeholder(tf.float32)\n",
    "        for i, _ in edge_types})\n",
    "    return placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5678f65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining placeholders\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining placeholders\")\n",
    "placeholders = construct_placeholders(edge_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e51508",
   "metadata": {},
   "source": [
    "# Defining Minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2cfeb741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeMinibatchIterator(object):\n",
    "    \"\"\" This minibatch iterator iterates over batches of sampled edges or\n",
    "    random pairs of co-occuring edges.\n",
    "    assoc -- numpy array with target edges\n",
    "    placeholders -- tensorflow placeholders object\n",
    "    batch_size -- size of the minibatches\n",
    "    \"\"\"\n",
    "    def __init__(self, adj_mats, feat, edge_types, batch_size=100, val_test_size=0.01):\n",
    "        self.adj_mats = adj_mats  #Cell 26\n",
    "        self.feat = feat           #{ 0: [12331, 17480 feature matrix], 1: [3215, 16592 feature matrix] }\n",
    "        self.edge_types = edge_types     #{(0, 0): 2, (0, 1): 1, (1, 0): 1, (1, 1): 2}\n",
    "        self.batch_size = batch_size         #512\n",
    "        self.val_test_size = val_test_size       #0.1\n",
    "        self.num_edge_types = sum(self.edge_types.values())        #6\n",
    "\n",
    "        self.iter = 0\n",
    "        self.freebatch_edge_types= list(range(self.num_edge_types))\n",
    "        self.batch_num = [0]*self.num_edge_types        #[0,0,0,0,0,0]\n",
    "        self.current_edge_type_idx = 0      \n",
    "        self.edge_type2idx = {}\n",
    "        self.idx2edge_type = {}\n",
    "        r = 0\n",
    "        for i, j in self.edge_types:\n",
    "            for k in range(self.edge_types[i,j]):\n",
    "                self.edge_type2idx[i, j, k] = r          #{(0,0,0): 0, (0,0,1): 1 ...}\n",
    "                self.idx2edge_type[r] = i, j, k          #{0: (0,0,0), 1: (0,0,1) ...}\n",
    "                r += 1\n",
    "\n",
    "        self.train_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()} #{(0,0):[None, None], ..}\n",
    "        self.val_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
    "        self.test_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
    "        self.test_edges_false = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
    "        self.val_edges_false = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
    "\n",
    "        # Function to build test and val sets with val_test_size positive links\n",
    "        self.adj_train = {edge_type: [None]*n for edge_type, n in self.edge_types.items()} #{(0,0):[None, None], ..}\n",
    "        for i, j in self.edge_types:\n",
    "            for k in range(self.edge_types[i,j]):\n",
    "                self.mask_test_edges((i, j), k)\n",
    "                \n",
    "    def mask_test_edges(self, edge_type, type_idx):\n",
    "        edges_all, _, _ = sparse_to_tuple(self.adj_mats[edge_type][type_idx])  #Cell 26 convert to {(n*2: x,y),n*val,shape} Cell 40\n",
    "        num_test = max(100, int(np.floor(edges_all.shape[0] * self.val_test_size)))  #edges_all are the coords in adj matrix\n",
    "        num_val = max(100, int(np.floor(edges_all.shape[0] * self.val_test_size)))   #num_test is max 100 ??? WHYYYY??\n",
    "\n",
    "        if edge_type not in [(0,1), (1, 0)]:   # DIS_DIS and GENE_GENE only 10 ???? WHYY??\n",
    "            num_test = 10\n",
    "            num_val = 10\n",
    "\n",
    "        all_edge_idx = list(range(edges_all.shape[0]))  #edges_all.shape[0] - > Num of non_zero values in sparse [0,1 .. N]\n",
    "        np.random.shuffle(all_edge_idx)\n",
    "\n",
    "        val_edge_idx = all_edge_idx[:num_val]\n",
    "        val_edges = edges_all[val_edge_idx]\n",
    "\n",
    "        test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
    "        test_edges = edges_all[test_edge_idx]\n",
    "\n",
    "        train_edges = np.delete(edges_all, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
    "\n",
    "        #Checks if edges are part of edges_all, then adds to test_edges_false if not there, same length as test_edges\n",
    "        test_edges_false = []\n",
    "        while len(test_edges_false) < len(test_edges):\n",
    "            idx_i = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[0])\n",
    "            idx_j = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[1])\n",
    "            if self._ismember([idx_i, idx_j], edges_all):\n",
    "                continue\n",
    "            if test_edges_false:\n",
    "                if self._ismember([idx_i, idx_j], test_edges_false):\n",
    "                    continue\n",
    "            test_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "        val_edges_false = []\n",
    "        while len(val_edges_false) < len(val_edges):\n",
    "            idx_i = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[0])\n",
    "            idx_j = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[1])\n",
    "            if self._ismember([idx_i, idx_j], edges_all):\n",
    "                continue\n",
    "            if val_edges_false:\n",
    "                if self._ismember([idx_i, idx_j], val_edges_false):\n",
    "                    continue\n",
    "            val_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "        #ALL _edges and _edges_false are arrays of coordinates [n*[x,y]]\n",
    "        # Re-build adj matrices\n",
    "        data = np.ones(train_edges.shape[0])\n",
    "        adj_train = sp.csr_matrix(\n",
    "            (data, (train_edges[:, 0], train_edges[:, 1])), \n",
    "            shape=self.adj_mats[edge_type][type_idx].shape)        #adj_train are now just 1's\n",
    "        self.adj_train[edge_type][type_idx] = self.preprocess_graph(adj_train)\n",
    "\n",
    "        self.train_edges[edge_type][type_idx] = train_edges\n",
    "        self.val_edges[edge_type][type_idx] = val_edges\n",
    "        self.val_edges_false[edge_type][type_idx] = np.array(val_edges_false)\n",
    "        self.test_edges[edge_type][type_idx] = test_edges\n",
    "        self.test_edges_false[edge_type][type_idx] = np.array(test_edges_false)\n",
    "        \n",
    "    def _ismember(self, a, b):\n",
    "        a = np.array(a)\n",
    "        b = np.array(b)\n",
    "        rows_close = np.all(a - b == 0, axis=1)  #Subtracts the pair a(idx_i, idx_j) from each element of b, if equal then \n",
    "        return np.any(rows_close)                #Function will return true if found a coordinates\n",
    "     \n",
    "    # Symteric (D^-0.5 . (A + I) . D^-0.5) and Assymteric (Dr ^ -0.5 . A . Dc ^ -0.5) normalization\n",
    "    # For A(mxm) -> adj_ = A + I(mxm), R(mx1) = Sum of rows of adj_, M_inv = D(mxm) (diaginals are roots of R), \n",
    "    # Adj_Normalized = (adj(mxm).D(mxm))'.D(mxm)\n",
    "    #For A(mxn) -> R(mx1) = sum of rows of A, C(nx1) = sum of cols of A, RD(mxm) -> Diagonals(non-null) are roots of R,\n",
    "    # CD(nxn) -> Diagonals(non-null) are roots of C  \n",
    "    # Adj_normalized = RD(mxm).A(mxn).CD(nxn) \n",
    "    def preprocess_graph(self, adj):\n",
    "        adj = sp.coo_matrix(adj)\n",
    "        if adj.shape[0] == adj.shape[1]:\n",
    "            adj_ = adj + sp.eye(adj.shape[0])\n",
    "            rowsum = np.array(adj_.sum(1))\n",
    "            degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "            adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "        else:\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "            colsum = np.array(adj.sum(0))\n",
    "            rowdegree_mat_inv = sp.diags(np.nan_to_num(np.power(rowsum, -0.5)).flatten())\n",
    "            coldegree_mat_inv = sp.diags(np.nan_to_num(np.power(colsum, -0.5)).flatten())\n",
    "            adj_normalized = rowdegree_mat_inv.dot(adj).dot(coldegree_mat_inv).tocoo()\n",
    "        return sparse_to_tuple(adj_normalized)\n",
    "    \n",
    "    def update_feed_dict(self, feed_dict, dropout, placeholders):\n",
    "        # construct feed dictionary\n",
    "        feed_dict.update({\n",
    "            placeholders['adj_mats_%d,%d,%d' % (i,j,k)]: self.adj_train[i,j][k]\n",
    "            for i, j in self.edge_types for k in range(self.edge_types[i,j])})\n",
    "        feed_dict.update({placeholders['feat_%d' % i]: self.feat[i] for i, _ in self.edge_types})\n",
    "        feed_dict.update({placeholders['dropout']: dropout})\n",
    "        return feed_dict\n",
    "    \n",
    "    def batch_feed_dict(self, batch_edges, batch_edge_type, placeholders):\n",
    "        feed_dict = dict()\n",
    "        feed_dict.update({placeholders['batch']: batch_edges})\n",
    "        feed_dict.update({placeholders['batch_edge_type_idx']: batch_edge_type})\n",
    "        feed_dict.update({placeholders['batch_row_edge_type']: self.idx2edge_type[batch_edge_type][0]})\n",
    "        feed_dict.update({placeholders['batch_col_edge_type']: self.idx2edge_type[batch_edge_type][1]})\n",
    "        return feed_dict\n",
    "    \n",
    "    def next_minibatch_feed_dict(self, placeholders):\n",
    "        \"\"\"Select a random edge type and a batch of edges of the same type\"\"\"\n",
    "        while True:\n",
    "            if self.iter % 4 == -1:\n",
    "                # gene-gene relation\n",
    "                self.current_edge_type_idx = self.edge_type2idx[0, 0, 0]\n",
    "            elif self.iter % 4 == 1:\n",
    "                # gene-drug relation\n",
    "                self.current_edge_type_idx = self.edge_type2idx[0, 1, 0]\n",
    "            elif self.iter % 4 == 2:\n",
    "                # drug-gene relation\n",
    "                self.current_edge_type_idx = self.edge_type2idx[1, 0, 0]\n",
    "            else:\n",
    "                # random side effect relation\n",
    "                if len(self.freebatch_edge_types) > 0:\n",
    "                    self.current_edge_type_idx = np.random.choice(self.freebatch_edge_types)\n",
    "                else:\n",
    "                    self.current_edge_type_idx = self.edge_type2idx[0, 1, 0]\n",
    "                    self.iter = 1\n",
    "\n",
    "            i, j, k = self.idx2edge_type[self.current_edge_type_idx]\n",
    "            if self.batch_num[self.current_edge_type_idx] * self.batch_size \\\n",
    "                   <= len(self.train_edges[i,j][k]) - self.batch_size + 1:\n",
    "                break\n",
    "            else:\n",
    "                if self.iter % 4 in [1, 2]:\n",
    "                    self.batch_num[self.current_edge_type_idx] = 0\n",
    "                else:\n",
    "                    self.freebatch_edge_types.remove(self.current_edge_type_idx)\n",
    "\n",
    "        self.iter += 1\n",
    "        start = self.batch_num[self.current_edge_type_idx] * self.batch_size\n",
    "        self.batch_num[self.current_edge_type_idx] += 1\n",
    "        batch_edges = self.train_edges[i,j][k][start: start + self.batch_size]\n",
    "        return self.batch_feed_dict(batch_edges, self.current_edge_type_idx, placeholders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0df506a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12331, 12331)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_num = [0]*sum(edge_types.values())\n",
    "a,b,c = sparse_to_tuple(adj_mats_orig[(0,0)][0])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5388e096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1115361/1749091881.py:120: RuntimeWarning: divide by zero encountered in power\n",
      "  rowdegree_mat_inv = sp.diags(np.nan_to_num(np.power(rowsum, -0.5)).flatten())\n",
      "/tmp/ipykernel_1115361/1749091881.py:121: RuntimeWarning: divide by zero encountered in power\n",
      "  coldegree_mat_inv = sp.diags(np.nan_to_num(np.power(colsum, -0.5)).flatten())\n"
     ]
    }
   ],
   "source": [
    "minibatch = EdgeMinibatchIterator(\n",
    "        adj_mats=adj_mats_orig,  #Check cell 26\n",
    "        feat=feat,              #{ 0: [12331, 17480 feature matrix], 1: [3215, 16592 feature matrix] }\n",
    "        edge_types=edge_types,   #{(0, 0): 2, (0, 1): 1, (1, 0): 1, (1, 1): 2}\n",
    "        batch_size=batch_size,      #512\n",
    "        val_test_size=val_test_size  #0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22197b58",
   "metadata": {},
   "source": [
    "##  Defining The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03006503",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
    "    \"\"\"Create a weight variable with Glorot & Bengio (AISTATS 2010)\n",
    "    initialization.\n",
    "    \"\"\"\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = tf.compat.v1.random_uniform([input_dim, output_dim], minval=-init_range,\n",
    "                                maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def weight_variable_xavier(input_dim, output_dim, name=\"\"):\n",
    "    W = tf.get_variable(name, shape=[input_dim, output_dim],\n",
    "               initializer=tf.contrib.layers.xavier_initializer())\n",
    "    return W\n",
    "\n",
    "def zeros(input_dim, output_dim, name=None):\n",
    "    \"\"\"All zeros.\"\"\"\n",
    "    initial = tf.zeros((input_dim, output_dim), dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def ones(input_dim, output_dim, name=None):\n",
    "    \"\"\"All zeros.\"\"\"\n",
    "    initial = tf.ones((input_dim, output_dim), dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5907434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs\n",
    "    \"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "\n",
    "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
    "    \"\"\"Dropout for sparse tensors. Currently fails for very large sparse tensors (>1M elements) # TODO: CHANGE THIS\n",
    "    \"\"\"\n",
    "    noise_shape = [num_nonzero_elems]\n",
    "    random_tensor = keep_prob         # 1 - dropout\n",
    "    random_tensor += tf.compat.v1.random_uniform(noise_shape)     #Add 1 - dropout to random matrix of Nx1, N is input \n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)    # If > 1, keep or else drop\n",
    "    pre_out = tf.compat.v1.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1./keep_prob)                             # The Remaining elements get multiplied, dropout concept\n",
    "\n",
    "\n",
    "class MultiLayer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "\n",
    "    # Properties    \n",
    "        name: String, defines the variable scope of the layer.\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_type=(), num_types=-1, **kwargs):\n",
    "        self.edge_type = edge_type\n",
    "        self.num_types = num_types\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "        self.issparse = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            outputs = self._call(inputs)\n",
    "            return outputs\n",
    "\n",
    "\n",
    "class GraphConvolutionSparseMulti(MultiLayer):\n",
    "    \"\"\"Graph convolution layer for sparse inputs.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, adj_mats,\n",
    "                 nonzero_feat, dropout=0., act=tf.nn.relu, **kwargs):\n",
    "        super(GraphConvolutionSparseMulti, self).__init__(**kwargs) #input_dim = num_feat:  {0: 17480, 1: 16592}\n",
    "        self.dropout = dropout\n",
    "        self.adj_mats = adj_mats\n",
    "        self.act = act\n",
    "        self.issparse = True\n",
    "        self.nonzero_feat = nonzero_feat  #nonzero_feat:  {0: 12331, 1: 3215}\n",
    "        with tf.compat.v1.variable_scope('%s_vars' % self.name):      #weights_i are glorot variables: input_dimxoutput_dim matrix\n",
    "            for k in range(self.num_types):\n",
    "                print(\"Initializing weights: weights_%d with shape: \" %k)\n",
    "                print(str(input_dim[self.edge_type[1]]) + \" \" + str(output_dim))\n",
    "                self.vars['weights_%d' % k] = weight_variable_glorot(\n",
    "                    input_dim[self.edge_type[1]], output_dim, name='weights_%d' % k) #input_dim:  {0: 17480, 1: 16592}\n",
    "                                                                                    #output_dim: hidden_1 units = 64\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        outputs = []\n",
    "        for k in range(self.num_types):\n",
    "            x = dropout_sparse(inputs, 1-self.dropout, self.nonzero_feat[self.edge_type[1]]) # x is shape 12331(3215) \n",
    "            x = tf.compat.v1.sparse_tensor_dense_matmul(x, self.vars['weights_%d' % k])       # x 12331 * []\n",
    "            x = tf.compat.v1.sparse_tensor_dense_matmul(self.adj_mats[self.edge_type][k], x)\n",
    "            outputs.append(self.act(x))\n",
    "        outputs = tf.add_n(outputs)\n",
    "        outputs = tf.nn.l2_normalize(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class GraphConvolutionMulti(MultiLayer):\n",
    "    \"\"\"Basic graph convolution layer for undirected graph without edge labels.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, adj_mats, dropout=0., act=tf.nn.relu, **kwargs):\n",
    "        super(GraphConvolutionMulti, self).__init__(**kwargs)\n",
    "        self.adj_mats = adj_mats\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        with tf.compat.v1.variable_scope('%s_vars' % self.name):\n",
    "            for k in range(self.num_types):\n",
    "                self.vars['weights_%d' % k] = weight_variable_glorot(\n",
    "                    input_dim, output_dim, name='weights_%d' % k)\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        outputs = []\n",
    "        for k in range(self.num_types):\n",
    "            x = tf.nn.dropout(inputs, 1-self.dropout)\n",
    "            x = tf.matmul(x, self.vars['weights_%d' % k])\n",
    "            x = tf.compat.v1.sparse_tensor_dense_matmul(self.adj_mats[self.edge_type][k], x)\n",
    "            outputs.append(self.act(x))\n",
    "        outputs = tf.add_n(outputs)\n",
    "        outputs = tf.nn.l2_normalize(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class DEDICOMDecoder(MultiLayer):\n",
    "    \"\"\"DEDICOM Tensor Factorization Decoder model layer for link prediction.\"\"\"\n",
    "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
    "        super(DEDICOMDecoder, self).__init__(**kwargs)\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        with tf.variable_scope('%s_vars' % self.name):\n",
    "            self.vars['global_interaction'] = weight_variable_glorot(\n",
    "                input_dim, input_dim, name='global_interaction')\n",
    "            for k in range(self.num_types):\n",
    "                tmp = weight_variable_glorot(\n",
    "                    input_dim, 1, name='local_variation_%d' % k)\n",
    "                self.vars['local_variation_%d' % k] = tf.reshape(tmp, [-1])\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        i, j = self.edge_type\n",
    "        outputs = []\n",
    "        for k in range(self.num_types):\n",
    "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
    "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
    "            relation = tf.diag(self.vars['local_variation_%d' % k])\n",
    "            product1 = tf.matmul(inputs_row, relation)\n",
    "            product2 = tf.matmul(product1, self.vars['global_interaction'])\n",
    "            product3 = tf.matmul(product2, relation)\n",
    "            rec = tf.matmul(product3, tf.transpose(inputs_col))\n",
    "            outputs.append(self.act(rec))\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class DistMultDecoder(MultiLayer):\n",
    "    \"\"\"DistMult Decoder model layer for link prediction.\"\"\"\n",
    "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
    "        super(DistMultDecoder, self).__init__(**kwargs)\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        with tf.variable_scope('%s_vars' % self.name):\n",
    "            for k in range(self.num_types):\n",
    "                tmp = weight_variable_glorot(\n",
    "                    input_dim, 1, name='relation_%d' % k)\n",
    "                self.vars['relation_%d' % k] = tf.reshape(tmp, [-1])\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        i, j = self.edge_type\n",
    "        outputs = []\n",
    "        for k in range(self.num_types):\n",
    "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
    "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
    "            relation = tf.diag(self.vars['relation_%d' % k])\n",
    "            intermediate_product = tf.matmul(inputs_row, relation)\n",
    "            rec = tf.matmul(intermediate_product, tf.transpose(inputs_col))\n",
    "            outputs.append(self.act(rec))\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BilinearDecoder(MultiLayer):\n",
    "    \"\"\"Bilinear Decoder model layer for link prediction.\"\"\"\n",
    "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
    "        super(BilinearDecoder, self).__init__(**kwargs)\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        with tf.variable_scope('%s_vars' % self.name):\n",
    "            for k in range(self.num_types):\n",
    "                self.vars['relation_%d' % k] = weight_variable_glorot(\n",
    "                    input_dim, input_dim, name='relation_%d' % k)\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        i, j = self.edge_type\n",
    "        outputs = []\n",
    "        for k in range(self.num_types):\n",
    "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
    "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
    "            intermediate_product = tf.matmul(inputs_row, self.vars['relation_%d' % k])\n",
    "            rec = tf.matmul(intermediate_product, tf.transpose(inputs_col))\n",
    "            outputs.append(self.act(rec))\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class InnerProductDecoder(MultiLayer):\n",
    "    \"\"\"Decoder model layer for link prediction.\"\"\"\n",
    "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
    "        super(InnerProductDecoder, self).__init__(**kwargs)\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        i, j = self.edge_type\n",
    "        outputs = []\n",
    "        for k in range(self.num_types):\n",
    "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
    "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
    "            rec = tf.matmul(inputs_row, tf.transpose(inputs_col))\n",
    "            outputs.append(self.act(rec))\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c9e1e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', True)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.compat.v1.variable_scope(self.name):\n",
    "            self._build()\n",
    "        variables = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class DecagonModel(Model):\n",
    "    def __init__(self, placeholders, num_feat, nonzero_feat, edge_types, decoders, **kwargs):\n",
    "        super(DecagonModel, self).__init__(**kwargs)\n",
    "        self.edge_types = edge_types    #{(0, 0): 2, (0, 1): 1, (1, 0): 1, (1, 1): 2}\n",
    "        self.num_edge_types = sum(self.edge_types.values())      #6\n",
    "        self.num_obj_types = max([i for i, _ in self.edge_types]) + 1       #2\n",
    "        self.decoders = decoders #{(0, 0): 'innerproduct', (0, 1): 'innerproduct', (1, 0): 'innerproduct', (1, 1): 'innerproduct'}    \n",
    "        self.inputs = {i: placeholders['feat_%d' % i] for i, _ in self.edge_types} #sparse tensors with n*indices, n*values, shape\n",
    "        self.input_dim = num_feat  #num_feat:  {0: 17480, 1: 16592}\n",
    "        self.nonzero_feat = nonzero_feat #nonzero_feat:  {0: 12331, 1: 3215}\n",
    "        self.placeholders = placeholders\n",
    "        self.dropout = placeholders['dropout']  #default 0\n",
    "        self.adj_mats = {et: [\n",
    "            placeholders['adj_mats_%d,%d,%d' % (et[0], et[1], k)] for k in range(n)]\n",
    "            for et, n in self.edge_types.items()}\n",
    "        self.build()\n",
    "\n",
    "    def _build(self):\n",
    "        self.hidden1 = defaultdict(list)  #Create empty list when trying to access key not there\n",
    "        for i, j in self.edge_types:\n",
    "            self.hidden1[i].append(GraphConvolutionSparseMulti(\n",
    "                input_dim=self.input_dim, output_dim=hidden1_units,\n",
    "                edge_type=(i,j), num_types=self.edge_types[i,j],         \n",
    "                adj_mats=self.adj_mats, nonzero_feat=self.nonzero_feat,\n",
    "                act=lambda x: x, dropout=self.dropout,       # no activation ??? \n",
    "                logging=self.logging)(self.inputs[j]))      # Logging is only true or false\n",
    "            # self.hidden1[i].append(GraphConvolutionMulti(\n",
    "            #     input_dim=self.input_dim, output_dim=hidden1_units,\n",
    "            #     edge_type=(i,j), num_types=self.edge_types[i,j],\n",
    "            #     adj_mats=self.adj_mats, act=lambda x: x, \n",
    "            #     dropout=self.dropout,logging=self.logging)(self.inputs[j]))\n",
    "\n",
    "        for i, hid1 in self.hidden1.items():\n",
    "            self.hidden1[i] = tf.nn.relu(tf.add_n(hid1))\n",
    "\n",
    "        self.embeddings_reltyp = defaultdict(list)\n",
    "        for i, j in self.edge_types:\n",
    "            self.embeddings_reltyp[i].append(GraphConvolutionMulti(\n",
    "                input_dim=hidden1_units, output_dim=hidden2_units,\n",
    "                edge_type=(i,j), num_types=self.edge_types[i,j],\n",
    "                adj_mats=self.adj_mats, act=lambda x: x,\n",
    "                dropout=self.dropout, logging=self.logging)(self.hidden1[j]))\n",
    "\n",
    "        self.embeddings = [None] * self.num_obj_types\n",
    "        for i, embeds in self.embeddings_reltyp.items():\n",
    "            # self.embeddings[i] = tf.nn.relu(tf.add_n(embeds))\n",
    "            self.embeddings[i] = tf.add_n(embeds)\n",
    "\n",
    "        self.edge_type2decoder = {}\n",
    "        for i, j in self.edge_types:\n",
    "            decoder = self.decoders[i, j]\n",
    "            if decoder == 'innerproduct':\n",
    "                self.edge_type2decoder[i, j] = InnerProductDecoder(\n",
    "                    input_dim=hidden2_units, logging=self.logging,\n",
    "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
    "                    act=lambda x: x, dropout=self.dropout)\n",
    "            elif decoder == 'distmult':\n",
    "                self.edge_type2decoder[i, j] = DistMultDecoder(\n",
    "                    input_dim=hidden2_units, logging=self.logging,\n",
    "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
    "                    act=lambda x: x, dropout=self.dropout)\n",
    "            elif decoder == 'bilinear':\n",
    "                self.edge_type2decoder[i, j] = BilinearDecoder(\n",
    "                    input_dim=hidden2_units, logging=self.logging,\n",
    "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
    "                    act=lambda x: x, dropout=self.dropout)\n",
    "            elif decoder == 'dedicom':\n",
    "                self.edge_type2decoder[i, j] = DEDICOMDecoder(\n",
    "                    input_dim=hidden2_units, logging=self.logging,\n",
    "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
    "                    act=lambda x: x, dropout=self.dropout)\n",
    "            else:\n",
    "                raise ValueError('Unknown decoder type')\n",
    "\n",
    "        self.latent_inters = []\n",
    "        self.latent_varies = []\n",
    "        for edge_type in self.edge_types:\n",
    "            decoder = self.decoders[edge_type]\n",
    "            for k in range(self.edge_types[edge_type]):\n",
    "                if decoder == 'innerproduct':\n",
    "                    glb = tf.eye(hidden2_units, hidden2_units)\n",
    "                    loc = tf.eye(hidden2_units, hidden2_units)\n",
    "                elif decoder == 'distmult':\n",
    "                    glb = tf.diag(self.edge_type2decoder[edge_type].vars['relation_%d' % k])\n",
    "                    loc = tf.eye(hidden2_units, hidden2_units)\n",
    "                elif decoder == 'bilinear':\n",
    "                    glb = self.edge_type2decoder[edge_type].vars['relation_%d' % k]\n",
    "                    loc = tf.eye(hidden2_units, hidden2_units)\n",
    "                elif decoder == 'dedicom':\n",
    "                    glb = self.edge_type2decoder[edge_type].vars['global_interaction']\n",
    "                    loc = tf.diag(self.edge_type2decoder[edge_type].vars['local_variation_%d' % k])\n",
    "                else:\n",
    "                    raise ValueError('Unknown decoder type')\n",
    "\n",
    "                self.latent_inters.append(glb)\n",
    "                self.latent_varies.append(loc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "49c6a5d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create model\n",
      "Initializing weights: weights_0 with shape: \n",
      "17480 64\n",
      "Initializing weights: weights_1 with shape: \n",
      "17480 64\n",
      "WARNING:tensorflow:From /home/indradutta/miniconda3/envs/GNNGeneDisease/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "Initializing weights: weights_0 with shape: \n",
      "16592 64\n",
      "Initializing weights: weights_0 with shape: \n",
      "17480 64\n",
      "Initializing weights: weights_0 with shape: \n",
      "16592 64\n",
      "Initializing weights: weights_1 with shape: \n",
      "16592 64\n"
     ]
    }
   ],
   "source": [
    "print(\"Create model\")\n",
    "model = DecagonModel(\n",
    "        placeholders=placeholders,\n",
    "        num_feat=num_feat,\n",
    "        nonzero_feat=nonzero_feat,\n",
    "        edge_types=edge_types,\n",
    "        decoders=edge_type2decoder,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1fe3c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecagonOptimizer(object):\n",
    "    def __init__(self, embeddings, latent_inters, latent_varies,\n",
    "                 degrees, edge_types, edge_type2dim, placeholders,\n",
    "                 margin=0.1, neg_sample_weights=1., batch_size=100):\n",
    "        self.embeddings= embeddings\n",
    "        self.latent_inters = latent_inters\n",
    "        self.latent_varies = latent_varies\n",
    "        self.edge_types = edge_types\n",
    "        self.degrees = degrees\n",
    "        self.edge_type2dim = edge_type2dim\n",
    "        self.obj_type2n = {i: self.edge_type2dim[i,j][0][0] for i, j in self.edge_types}\n",
    "        self.margin = margin\n",
    "        self.neg_sample_weights = neg_sample_weights\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.inputs = placeholders['batch']\n",
    "        self.batch_edge_type_idx = placeholders['batch_edge_type_idx']\n",
    "        self.batch_row_edge_type = placeholders['batch_row_edge_type']\n",
    "        self.batch_col_edge_type = placeholders['batch_col_edge_type']\n",
    "\n",
    "        self.row_inputs = tf.squeeze(gather_cols(self.inputs, [0]))\n",
    "        self.col_inputs = tf.squeeze(gather_cols(self.inputs, [1]))\n",
    "\n",
    "        obj_type_n = [self.obj_type2n[i] for i in range(len(self.embeddings))]\n",
    "        self.obj_type_lookup_start = tf.cumsum([0] + obj_type_n[:-1])\n",
    "        self.obj_type_lookup_end = tf.cumsum(obj_type_n)\n",
    "\n",
    "        labels = tf.reshape(tf.cast(self.row_inputs, dtype=tf.int64), [self.batch_size, 1])\n",
    "        neg_samples_list = []\n",
    "        for i, j in self.edge_types:\n",
    "            for k in range(self.edge_types[i,j]):\n",
    "                neg_samples, _, _ = tf.nn.fixed_unigram_candidate_sampler(\n",
    "                    true_classes=labels,\n",
    "                    num_true=1,\n",
    "                    num_sampled=self.batch_size,\n",
    "                    unique=False,\n",
    "                    range_max=len(self.degrees[i][k]),\n",
    "                    distortion=0.75,\n",
    "                    unigrams=self.degrees[i][k].tolist())\n",
    "                neg_samples_list.append(neg_samples)\n",
    "        self.neg_samples = tf.gather(neg_samples_list, self.batch_edge_type_idx)\n",
    "\n",
    "        self.preds = self.batch_predict(self.row_inputs, self.col_inputs)\n",
    "        self.outputs = tf.compat.v1.diag_part(self.preds)\n",
    "        self.outputs = tf.reshape(self.outputs, [-1])\n",
    "\n",
    "        self.neg_preds = self.batch_predict(self.neg_samples, self.col_inputs)\n",
    "        self.neg_outputs = tf.compat.v1.diag_part(self.neg_preds)\n",
    "        self.neg_outputs = tf.reshape(self.neg_outputs, [-1])\n",
    "\n",
    "        self.predict()\n",
    "\n",
    "        self._build()\n",
    "\n",
    "    def batch_predict(self, row_inputs, col_inputs):\n",
    "        concatenated = tf.concat(self.embeddings, 0)\n",
    "\n",
    "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_row_edge_type)\n",
    "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_row_edge_type)\n",
    "        indices = tf.range(ind_start, ind_end)\n",
    "        row_embeds = tf.gather(concatenated, indices)\n",
    "        row_embeds = tf.gather(row_embeds, row_inputs)\n",
    "\n",
    "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_col_edge_type)\n",
    "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_col_edge_type)\n",
    "        indices = tf.range(ind_start, ind_end)\n",
    "        col_embeds = tf.gather(concatenated, indices)\n",
    "        col_embeds = tf.gather(col_embeds, col_inputs)\n",
    "\n",
    "        latent_inter = tf.gather(self.latent_inters, self.batch_edge_type_idx)\n",
    "        latent_var = tf.gather(self.latent_varies, self.batch_edge_type_idx)\n",
    "\n",
    "        product1 = tf.matmul(row_embeds, latent_var)\n",
    "        product2 = tf.matmul(product1, latent_inter)\n",
    "        product3 = tf.matmul(product2, latent_var)\n",
    "        preds = tf.matmul(product3, tf.transpose(col_embeds))\n",
    "        return preds\n",
    "\n",
    "    def predict(self):\n",
    "        concatenated = tf.concat(self.embeddings, 0)\n",
    "\n",
    "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_row_edge_type)\n",
    "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_row_edge_type)\n",
    "        indices = tf.range(ind_start, ind_end)\n",
    "        row_embeds = tf.gather(concatenated, indices)\n",
    "\n",
    "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_col_edge_type)\n",
    "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_col_edge_type)\n",
    "        indices = tf.range(ind_start, ind_end)\n",
    "        col_embeds = tf.gather(concatenated, indices)\n",
    "\n",
    "        latent_inter = tf.gather(self.latent_inters, self.batch_edge_type_idx)\n",
    "        latent_var = tf.gather(self.latent_varies, self.batch_edge_type_idx)\n",
    "\n",
    "        product1 = tf.matmul(row_embeds, latent_var)\n",
    "        product2 = tf.matmul(product1, latent_inter)\n",
    "        product3 = tf.matmul(product2, latent_var)\n",
    "        self.predictions = tf.matmul(product3, tf.transpose(col_embeds))\n",
    "\n",
    "    def _build(self):\n",
    "        self.cost = self._hinge_loss(self.outputs, self.neg_outputs)\n",
    "        # self.cost = self._xent_loss(self.outputs, self.neg_outputs)\n",
    "        self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.cost)\n",
    "        self.grads_vars = self.optimizer.compute_gradients(self.cost)\n",
    "\n",
    "    def _hinge_loss(self, aff, neg_aff):\n",
    "        \"\"\"Maximum-margin optimization using the hinge loss.\"\"\"\n",
    "        diff = tf.nn.relu(tf.subtract(neg_aff, tf.expand_dims(aff, 0) - self.margin), name='diff')\n",
    "        loss = tf.reduce_sum(diff)\n",
    "        return loss\n",
    "\n",
    "    def _xent_loss(self, aff, neg_aff):\n",
    "        \"\"\"Cross-entropy optimization.\"\"\"\n",
    "        true_xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(aff), logits=aff)\n",
    "        negative_xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(neg_aff), logits=neg_aff)\n",
    "        loss = tf.reduce_sum(true_xent) + self.neg_sample_weights * tf.reduce_sum(negative_xent)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def gather_cols(params, indices, name=None):\n",
    "    \"\"\"Gather columns of a 2D tensor.\n",
    "\n",
    "    Args:\n",
    "        params: A 2D tensor.\n",
    "        indices: A 1D tensor. Must be one of the following types: ``int32``, ``int64``.\n",
    "        name: A name for the operation (optional).\n",
    "\n",
    "    Returns:\n",
    "        A 2D Tensor. Has the same type as ``params``.\n",
    "    \"\"\"\n",
    "    with tf.compat.v1.op_scope([params, indices], name, \"gather_cols\") as scope:\n",
    "        # Check input\n",
    "        params = tf.convert_to_tensor(params, name=\"params\")\n",
    "        indices = tf.convert_to_tensor(indices, name=\"indices\")\n",
    "        try:\n",
    "            params.get_shape().assert_has_rank(2)\n",
    "        except ValueError:\n",
    "            raise ValueError('\\'params\\' must be 2D.')\n",
    "        try:\n",
    "            indices.get_shape().assert_has_rank(1)\n",
    "        except ValueError:\n",
    "            raise ValueError('\\'params\\' must be 1D.')\n",
    "\n",
    "        # Define op\n",
    "        p_shape = tf.shape(params)\n",
    "        p_flat = tf.reshape(params, [-1])\n",
    "        i_flat = tf.reshape(tf.reshape(tf.range(0, p_shape[0]) * p_shape[1],\n",
    "                                       [-1, 1]) + indices, [-1])\n",
    "        return tf.reshape(\n",
    "            tf.gather(p_flat, i_flat), [p_shape[0], -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45657796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create optimizer\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n"
     ]
    }
   ],
   "source": [
    "print(\"Create optimizer\")\n",
    "with tf.name_scope('optimizer'):\n",
    "        opt = DecagonOptimizer(\n",
    "            embeddings=model.embeddings,\n",
    "            latent_inters=model.latent_inters,\n",
    "            latent_varies=model.latent_varies,\n",
    "            degrees=degrees,\n",
    "            edge_types=edge_types,\n",
    "            edge_type2dim=edge_type2dim,\n",
    "            placeholders=placeholders,\n",
    "            batch_size=batch_size,\n",
    "            margin=max_margin\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e0874af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize session\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 12:51:56.002848: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-22 12:51:56.003071: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-22 12:51:56.003226: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-22 12:51:56.552962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-22 12:51:56.553152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-22 12:51:56.553306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-22 12:51:56.553437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11387 MB memory:  -> device: 0, name: NVIDIA TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2023-11-22 12:51:56.581722: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "print(\"Initialize session\")\n",
    "sess = tf.compat.v1.Session()\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "feed_dict = {}\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "saver.restore(sess,'./model/model.ckpt')\n",
    "feed_dict = minibatch.next_minibatch_feed_dict(placeholders=placeholders)\n",
    "feed_dict = minibatch.update_feed_dict(\n",
    "        feed_dict=feed_dict,\n",
    "        dropout=dropout,\n",
    "        placeholders=placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f2695d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bedroc_score(y_true, y_pred, decreasing=True, alpha=20.0):\n",
    "\n",
    "    \"\"\"BEDROC metric implemented according to Truchon and Bayley.\n",
    "\n",
    "    The Boltzmann Enhanced Descrimination of the Receiver Operator\n",
    "    Characteristic (BEDROC) score is a modification of the Receiver Operator\n",
    "    Characteristic (ROC) score that allows for a factor of *early recognition*.\n",
    "\n",
    "    References:\n",
    "        The original paper by Truchon et al. is located at `10.1021/ci600426e\n",
    "        <http://dx.doi.org/10.1021/ci600426e>`_.\n",
    "\n",
    "    Args:\n",
    "        y_true (array_like):\n",
    "            Binary class labels. 1 for positive class, 0 otherwise.\n",
    "        y_pred (array_like):\n",
    "            Prediction values.\n",
    "        decreasing (bool):\n",
    "            True if high values of ``y_pred`` correlates to positive class.\n",
    "        alpha (float):\n",
    "            Early recognition parameter.\n",
    "\n",
    "    Returns:\n",
    "        float:\n",
    "            Value in interval [0, 1] indicating degree to which the predictive\n",
    "            technique employed detects (early) the positive class.\n",
    "     \"\"\"\n",
    "\n",
    "    assert len(y_true) == len(y_pred), \\\n",
    "        'The number of scores must be equal to the number of labels'\n",
    "\n",
    "    big_n = len(y_true)\n",
    "    n = sum(y_true == 1)\n",
    "\n",
    "    if decreasing:\n",
    "        order = np.argsort(-y_pred)\n",
    "    else:\n",
    "        order = np.argsort(y_pred)\n",
    "\n",
    "    m_rank = (y_true[order] == 1).nonzero()[0]\n",
    "\n",
    "    s = np.sum(np.exp(-alpha * m_rank / big_n))\n",
    "\n",
    "    r_a = n / big_n\n",
    "\n",
    "    rand_sum = r_a * (1 - np.exp(-alpha))/(np.exp(alpha/big_n) - 1)\n",
    "\n",
    "    fac = r_a * np.sinh(alpha / 2) / (np.cosh(alpha / 2) -\n",
    "                                      np.cosh(alpha/2 - alpha * r_a))\n",
    "\n",
    "    cte = 1 / (1 - np.exp(alpha * (1 - r_a)))\n",
    "\n",
    "    return s * fac / rand_sum + cte\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58fa5732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "\n",
    "    This function computes the average precision at k between two lists of\n",
    "    items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "\n",
    "def ark(actual, predicted, k=10):\n",
    "\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    num_actual = len(actual)\n",
    "\n",
    "    num_hits = 0.0\n",
    "    if len(actual)==0:\n",
    "        return 0\n",
    "\n",
    "    for i, p in enumerate(actual):\n",
    "        if p in predicted:\n",
    "            num_hits += 1.0\n",
    "\n",
    "\n",
    "    return num_hits / len(actual)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "\n",
    "    This function computes the mean average precision at k between two lists\n",
    "    of lists of items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted\n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a, p in zip(actual, predicted)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f2166886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_scores(edges_pos, edges_neg, edge_type, name=None):\n",
    "    feed_dict.update({placeholders['dropout']: 0})\n",
    "    feed_dict.update({placeholders['batch_edge_type_idx']: minibatch.edge_type2idx[edge_type]})\n",
    "    feed_dict.update({placeholders['batch_row_edge_type']: edge_type[0]})\n",
    "    feed_dict.update({placeholders['batch_col_edge_type']: edge_type[1]})\n",
    "    rec = sess.run(opt.predictions, feed_dict=feed_dict)\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1. / (1 + np.exp(-x))\n",
    "\n",
    "    preds = []\n",
    "    actual = []\n",
    "    predicted = []\n",
    "    edge_ind = 0\n",
    "    for u, v in edges_pos[edge_type[:2]][edge_type[2]]:\n",
    "        score = sigmoid(rec[u, v])\n",
    "        preds.append(score)\n",
    "\n",
    "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 1, 'Problem 1'\n",
    "\n",
    "        actual.append(edge_ind)\n",
    "        predicted.append((score, edge_ind))\n",
    "        edge_ind += 1\n",
    "\n",
    "    preds_neg = []\n",
    "    for u, v in edges_neg[edge_type[:2]][edge_type[2]]:\n",
    "        score = sigmoid(rec[u, v])\n",
    "        preds_neg.append(score)\n",
    "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 0, 'Problem 0'\n",
    "\n",
    "        predicted.append((score, edge_ind))\n",
    "        edge_ind += 1\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    preds_all = np.nan_to_num(preds_all)\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
    "    predicted = list(zip(*sorted(predicted, reverse=True, key=itemgetter(0))))[1]\n",
    "\n",
    "    roc_sc = metrics.roc_auc_score(labels_all, preds_all)\n",
    "    aupr_sc = metrics.average_precision_score(labels_all, preds_all)\n",
    "    apk_sc = apk(actual, predicted, k=200)\n",
    "    bedroc_sc = bedroc_score(labels_all, preds_all)\n",
    "    if name!=None:\n",
    "        with open(name, 'wb') as f:\n",
    "            pickle.dump([labels_all, preds_all], f)\n",
    "    return roc_sc, aupr_sc, apk_sc, bedroc_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c47f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(edges_pos, edges_neg, edge_type):\n",
    "    feed_dict.update({placeholders['dropout']: 0})\n",
    "    feed_dict.update({placeholders['batch_edge_type_idx']: minibatch.edge_type2idx[edge_type]})\n",
    "    feed_dict.update({placeholders['batch_row_edge_type']: edge_type[0]})\n",
    "    feed_dict.update({placeholders['batch_col_edge_type']: edge_type[1]})\n",
    "    rec = sess.run(opt.predictions, feed_dict=feed_dict)\n",
    "\n",
    "    return 1. / (1 + np.exp(-rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3012e297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge type= [01, 00, 00]\n",
      "Edge type: 0003 Test AUROC score 0.50000\n",
      "Edge type: 0003 Test AUPRC score 0.50000\n",
      "Edge type: 0003 Test AP@k score 1.00000\n",
      "Edge type: 0003 Test BEDROC score 0.02564\n",
      "\n",
      "Saving result...\n"
     ]
    }
   ],
   "source": [
    "roc_score, auprc_score, apk_score, bedroc = get_accuracy_scores(\n",
    "        minibatch.test_edges, minibatch.test_edges_false, minibatch.idx2edge_type[3])\n",
    "print(\"Edge type=\", \"[%02d, %02d, %02d]\" % minibatch.idx2edge_type[3])\n",
    "print(\"Edge type:\", \"%04d\" % 3, \"Test AUROC score\", \"{:.5f}\".format(roc_score))\n",
    "print(\"Edge type:\", \"%04d\" % 3, \"Test AUPRC score\", \"{:.5f}\".format(auprc_score))\n",
    "print(\"Edge type:\", \"%04d\" % 3, \"Test AP@k score\", \"{:.5f}\".format(apk_score))\n",
    "print(\"Edge type:\", \"%04d\" % 3, \"Test BEDROC score\", \"{:.5f}\".format(bedroc))\n",
    "print()\n",
    "\n",
    "prediction = get_prediction(minibatch.test_edges, minibatch.test_edges_false, \n",
    "    \tminibatch.idx2edge_type[3])\n",
    "\n",
    "print('Saving result...')\n",
    "np.save('prediction.npy', prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
